# Stage 3: Evidence Extraction ‚Äî UX First Principles Analysis

**Analysis Date:** 2025-11-14
**Analyst:** Claude Code Agent
**Stage:** Evidence Extraction (Stage 3 of 7)
**Status:** Draft ‚Äî Ready for Review

---

## Executive Summary

Evidence Extraction represents the most cognitively demanding and cost-intensive stage of the Registry Review workflow. This stage transforms discovered documents from mere files into structured, requirement-mapped evidence with confidence scores. The UX challenges center around managing ambiguity, communicating costs transparently, surfacing extraction failures gracefully, and enabling Becca to verify and augment machine-extracted evidence efficiently.

Current implementation uses keyword-based extraction with markdown parsing. Future evolution toward LLM-powered extraction introduces new UX considerations around cost tracking, confidence calibration, and cache transparency.

**Critical UX Insight:** Evidence extraction is where the agent earns trust or loses it. False positives create busywork for reviewers. False negatives create compliance risks. The interface must make confidence levels immediately interpretable and provide escape hatches for manual intervention.

---

## 1. Stage Overview

### Purpose
Map each checklist requirement to relevant documents and extract evidence snippets demonstrating compliance.

### Current Implementation
- **Keyword-Based Extraction** (`evidence_tools.py`): Extracts keywords from requirement text, searches markdown content, calculates relevance scores, extracts contextual snippets with page/section metadata
- **Coverage Categorization**: covered (high confidence evidence), partial (some evidence but incomplete), missing (no evidence found), flagged (extraction errors)
- **Caching Layer**: PDF conversion results cached to avoid re-processing
- **Cost Tracking**: Comprehensive tracking for LLM-based extractors (dates, land tenure, project IDs)

### Future Evolution (LLM-Enhanced)
The codebase includes LLM extractors (`llm_extractors.py`) for structured field extraction:
- **Date Extraction**: Project start dates, crediting periods, imagery dates, sampling dates
- **Land Tenure Extraction**: Owner names, area calculations, tenure types
- **Project ID Extraction**: Cross-document ID consistency validation

These extractors introduce new UX requirements around cost transparency and confidence interpretation.

---

## 2. User Experience Framework Analysis

### A. Information Architecture

**How is evidence organized and presented?**

**Current Structure:**
```
Evidence Result
‚îú‚îÄ‚îÄ Summary Statistics (requirements_total, covered, partial, missing, flagged, overall_coverage)
‚îú‚îÄ‚îÄ Evidence List (per requirement)
‚îÇ   ‚îú‚îÄ‚îÄ Requirement Metadata (id, text, category)
‚îÇ   ‚îú‚îÄ‚îÄ Status (covered | partial | missing | flagged)
‚îÇ   ‚îú‚îÄ‚îÄ Confidence Score (0.0 - 1.0)
‚îÇ   ‚îú‚îÄ‚îÄ Mapped Documents (document_id, relevance_score, keywords_found)
‚îÇ   ‚îî‚îÄ‚îÄ Evidence Snippets
‚îÇ       ‚îú‚îÄ‚îÄ Text (contextual snippet ~100 words)
‚îÇ       ‚îú‚îÄ‚îÄ Document Reference (name, id)
‚îÇ       ‚îú‚îÄ‚îÄ Location (page number, section header)
‚îÇ       ‚îî‚îÄ‚îÄ Confidence (snippet-level score)
‚îî‚îÄ‚îÄ Metadata (session_id, extracted_at)
```

**Strengths:**
- Clear hierarchical organization from summary to detail
- Multiple levels of granularity (overall ‚Üí requirement ‚Üí snippet)
- Rich location metadata enables citation verification

**Weaknesses:**
- No indication of *why* evidence is partial vs. covered
- Confidence scores lack context (is 0.72 good or bad?)
- Missing visual hierarchy in console output (all text-based)
- No way to compare evidence across similar requirements

**UX Improvements:**

1. **Contextual Confidence Interpretation**
   ```
   Confidence: 0.92 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê STRONG
   Confidence: 0.72 ‚≠ê‚≠ê‚≠ê‚ö™‚ö™ MODERATE - Review recommended
   Confidence: 0.45 ‚≠ê‚ö™‚ö™‚ö™‚ö™ WEAK - Manual verification required
   ```

2. **Evidence Quality Indicators**
   ```
   REQ-007: Project Start Date
   Status: Covered ‚úÖ
   Confidence: 0.95 (Strong)

   Why covered?
   ‚úì Explicit date statement found
   ‚úì Multiple document confirmation (3 sources)
   ‚úì No conflicting dates detected

   Evidence: "Project commenced January 1, 2022" (Project Plan, p.4)
   ```

3. **Partial Evidence Explanation**
   ```
   REQ-015: Baseline Soil Carbon Measurements
   Status: Partial ‚ö†Ô∏è
   Confidence: 0.58 (Moderate - needs review)

   What's missing?
   ‚úì Sampling methodology described
   ‚úì Sample locations specified
   ‚úó Actual measurement results not found
   ‚úó Lab analysis reports missing

   Found evidence suggests requirement is partially addressed.
   Consider: Check for separate lab report documents.
   ```

4. **Visual Hierarchy for Console Output**
   ```
   ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
   ‚ïë  EVIDENCE EXTRACTION RESULTS                                  ‚ïë
   ‚ïë  Session: session-botany-farm-2023                           ‚ïë
   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

   üìä COVERAGE SUMMARY
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Total Requirements:  23                                      ‚îÇ
   ‚îÇ ‚úÖ Covered:          18 (78%)  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë       ‚îÇ
   ‚îÇ ‚ö†Ô∏è  Partial:           3 (13%)  ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë        ‚îÇ
   ‚îÇ ‚ùå Missing:           2 (9%)   ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë         ‚îÇ
   ‚îÇ üö© Flagged:           0 (0%)   ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ```

### B. Interaction Design

**What actions can users take at this stage?**

**Current Actions:**
1. Run evidence extraction (`/evidence-extraction`)
2. View extraction results (passive - displayed after completion)
3. Re-run extraction (idempotent - overwrites previous results)

**Missing Interactions:**
- Cannot filter/sort evidence by confidence or status
- Cannot manually add evidence snippets
- Cannot adjust confidence scores based on domain knowledge
- Cannot mark evidence as verified/rejected
- Cannot request re-extraction for specific requirements only
- No preview before committing extraction results

**Proposed Interaction Enhancements:**

1. **Selective Re-Extraction**
   ```
   Command: /evidence-extraction --requirements REQ-015,REQ-018

   Re-extracting evidence for 2 requirements:
   - REQ-015: Baseline soil carbon measurements
   - REQ-018: Additionality demonstration

   This will preserve evidence for other requirements.
   Continue? [Y/n]
   ```

2. **Manual Evidence Addition Workflow**
   ```
   Command: /add-evidence REQ-015 --document monitoring_report.pdf --page 12

   Opening document viewer:
   [Display: monitoring_report.pdf, page 12]

   Highlight evidence text or describe location:
   > "Table 3 shows baseline SOC measurements"

   Confidence level (0.0-1.0): 1.0
   Note (optional): "Manual verification from monitoring report"

   ‚úÖ Evidence added to REQ-015
   Status updated: missing ‚Üí covered
   ```

3. **Evidence Review Interface** (Future Web UI)
   ```
   [Left Panel: Requirements List]
   ‚úÖ REQ-001: Project name documented
   ‚úÖ REQ-002: Project location specified
   ‚ö†Ô∏è  REQ-015: Baseline measurements [REVIEW]  ‚Üê Selected

   [Right Panel: Evidence Details]
   REQ-015: Baseline Soil Carbon Measurements
   Status: Partial
   Confidence: 0.58

   Mapped Documents (2):
   [x] baseline_report.pdf (relevance: 0.85)
   [x] monitoring_plan.pdf (relevance: 0.72)

   Evidence Snippets (3):
   [‚úì] "Sampling occurred January 2022" - baseline_report.pdf p.4
       Confidence: 0.82 | [Edit] [Remove] [Verify]

   [‚ö†Ô∏è] "Methodology follows protocol v1.2.2" - monitoring_plan.pdf p.8
       Confidence: 0.45 | [Edit] [Remove] [Verify]
       Note: Mentions methodology but not actual measurements

   [Add Manual Evidence] [Request Re-Extraction] [Mark as Verified]
   ```

4. **Confidence Calibration Feedback Loop**
   ```
   After human review, Becca can mark evidence:
   - ‚úÖ Verified: "This evidence correctly supports the requirement"
   - ‚ùå Incorrect: "This evidence doesn't actually support the requirement"
   - ‚ö†Ô∏è  Unclear: "This evidence is ambiguous"

   System learns from feedback:
   "You've marked 3 snippets with confidence > 0.8 as incorrect.
    Adjusting confidence threshold for similar patterns."
   ```

### C. Visual Design

**Current Visual Language:**

Console output uses:
- Text symbols: ‚úÖ ‚ö†Ô∏è ‚ùå üö©
- Percentage bars (text-based)
- Truncated text: "requirement_text[:80]..."
- Section headers with `===` separators

**Visual Design Challenges:**

1. **Long Evidence Snippets**: Snippets can be ~100 words, making console output overwhelming
2. **No Document Preview**: Cannot see original document context
3. **Limited Comparison**: Hard to compare evidence across requirements visually
4. **Confidence Ambiguity**: Numerical scores (0.72) lack intuitive meaning

**Visual Design Improvements:**

1. **Collapsible/Expandable Evidence**
   ```
   ‚ö†Ô∏è  REQ-015: Baseline soil carbon measurements [‚ñº Show 3 snippets]
       Status: Partial | Confidence: 0.58

       [Click to expand]

   ‚ö†Ô∏è  REQ-015: Baseline soil carbon measurements [‚ñ≤ Hide snippets]
       Status: Partial | Confidence: 0.58

       Snippet 1/3 (monitoring_report.pdf, p.12)
       Confidence: 0.72 ‚≠ê‚≠ê‚≠ê‚ö™‚ö™
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ "Soil sampling was conducted across all project         ‚îÇ
       ‚îÇ  parcels in January 2022. Samples were collected at     ‚îÇ
       ‚îÇ  depths of 0-10cm, 10-30cm following protocol..."       ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       [View Full Document] [Edit] [Remove]
   ```

2. **Visual Confidence Indicators**
   - Color coding: Green (>0.8), Yellow (0.5-0.8), Red (<0.5)
   - Icon sets: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (0.9+), ‚≠ê‚≠ê‚≠ê‚ö™‚ö™ (0.6-0.8)
   - Progress bars: `[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 82%`

3. **Evidence Coverage Heatmap** (Future Web UI)
   ```
   Requirements Coverage Matrix:

   Category          | Requirements | Covered | Partial | Missing
   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   Project Details   | 5           | ‚ñà‚ñà‚ñà‚ñà 4  | ‚ñà 1    | - 0
   Land Eligibility  | 8           | ‚ñà‚ñà 2    | ‚ñà‚ñà‚ñà 3  | ‚ñà‚ñà‚ñà 3
   Baseline Data     | 6           | ‚ñà‚ñà‚ñà‚ñà 4  | ‚ñà 1    | ‚ñà 1
   Monitoring        | 4           | ‚ñà‚ñà‚ñà‚ñà 4  | - 0    | - 0
   ```

### D. Content Strategy

**What information needs to be communicated?**

**Essential Information:**
1. Overall coverage statistics (how much is done)
2. Per-requirement status (what's covered, what's not)
3. Confidence levels (how reliable is the extraction)
4. Evidence location (where to verify in documents)
5. Missing evidence guidance (what to look for)

**Current Content Strengths:**
- Clear numerical summary (23 total, 18 covered, etc.)
- Requirement-by-requirement breakdown
- Document citations with page numbers
- Confidence scores provided

**Content Gaps:**

1. **No Contextual Guidance**: System doesn't explain *why* evidence is missing
2. **No Action Recommendations**: Doesn't suggest next steps for partial/missing evidence
3. **No Comparison to Typical Projects**: Is 78% coverage good for this methodology?
4. **No Risk Assessment**: Which missing requirements are critical vs. nice-to-have?

**Content Strategy Improvements:**

1. **Actionable Next Steps**
   ```
   ‚ùå MISSING REQUIREMENTS (2) - NEEDS ATTENTION:

   REQ-015: Baseline soil carbon measurements
   Category: Baseline Data
   Priority: üî¥ CRITICAL (required for crediting)

   What to do:
   1. Check for separate lab analysis reports
   2. Look for appendices in monitoring documents
   3. Contact project developer if still missing

   Common document names: "Lab Results", "SOC Analysis", "Baseline Report Appendix A"
   ```

2. **Comparative Benchmarks**
   ```
   üìä Coverage Summary:
   Overall Coverage: 78%

   Benchmark: For Soil Carbon v1.2.2 projects:
   - Typical coverage: 85-90%
   - Minimum acceptable: 75%
   - Your project: 78% ‚úÖ ACCEPTABLE

   Note: 2 missing requirements are in "Baseline Data" category.
   This is common for early-stage submissions. Consider requesting
   additional documentation from project developer.
   ```

3. **Risk-Based Prioritization**
   ```
   üö© HIGH PRIORITY GAPS (must address):
   - REQ-015: Baseline measurements (CRITICAL for crediting)

   ‚ö†Ô∏è  MEDIUM PRIORITY GAPS (should address):
   - REQ-018: Additionality demonstration (needed for verification)

   ‚úÖ All critical project identification requirements covered
   ```

4. **Evidence Quality Narrative**
   ```
   Evidence Quality Assessment:

   Strong Evidence (Confidence > 0.8): 15 requirements
   - These requirements have explicit, multi-document confirmation
   - Low risk of compliance issues

   Moderate Evidence (0.5 - 0.8): 6 requirements
   - Evidence found but may need verification
   - Recommend manual review before approval

   Weak Evidence (< 0.5): 2 requirements
   - Evidence is ambiguous or circumstantial
   - MUST be manually verified or re-extracted
   ```

### E. Accessibility

**Who can use this feature and how?**

**Current Accessibility:**
- CLI-based interface (text output only)
- No visual dependencies (works in screen readers)
- Console output is copy-pasteable for documentation

**Accessibility Concerns:**

1. **Overwhelming Text Volume**: 23 requirements √ó 3 snippets = 69 text blocks
2. **No Progressive Disclosure**: All information displayed at once
3. **Difficult to Navigate**: No keyboard shortcuts or jump-to-requirement
4. **Color Dependence** (future): If using color for confidence, need alternative indicators

**Accessibility Improvements:**

1. **Progressive Disclosure**
   ```
   üìä COVERAGE SUMMARY (5 lines)
   [Press SPACE to expand sections]

   ‚ñ∂ Covered Requirements (18) [Press ENTER]
   ‚ñ∂ Partial Requirements (3) [Press ENTER]
   ‚ñ∂ Missing Requirements (2) [Press ENTER] ‚Üê Auto-expanded
   ‚ñ∂ Flagged Requirements (0) [Press ENTER]
   ```

2. **Keyboard Navigation**
   ```
   Commands available:
   - 'n' - Next requirement
   - 'p' - Previous requirement
   - 'c' - Jump to covered requirements
   - 'm' - Jump to missing requirements
   - 'f' - Filter by category
   - 'q' - Return to summary
   ```

3. **Screen Reader Friendly**
   ```
   Instead of: "REQ-015 ‚ö†Ô∏è  Partial (0.58)"
   Use: "Requirement 015: Partial status, confidence score 0.58"

   Instead of: "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë 82%"
   Use: "Coverage: 82 percent, 18 of 23 requirements"
   ```

4. **Export Options**
   ```
   Save extraction results:
   1. Human-readable text (screen reader friendly)
   2. Structured JSON (for programmatic access)
   3. CSV (for spreadsheet analysis)
   4. Markdown (for documentation)

   > /evidence-extraction --export-format markdown
   ‚úÖ Saved to: evidence_report.md
   ```

---

## 3. Stage-Specific Deep Dives

### A. LLM Extraction Failure Modes

**How does LLM extraction handle ambiguity?**

**Failure Mode 1: Malformed JSON Response**

*Scenario:* LLM returns invalid JSON or non-JSON text.

*Current Handling:* (`llm_extractors.py:553-569`)
```python
try:
    json_str = extract_json_from_response(response_text)
    extracted_data = validate_and_parse_extraction_response(json_str, "date")
    chunk_fields = [ExtractedField(**data) for data in extracted_data]
except ValueError as e:
    logger.error(f"Invalid response from LLM for {chunk_name}: {e}")
    return []  # Return empty list, continue with other chunks
```

*UX Impact:*
- Extraction silently fails for that chunk
- User sees fewer snippets than expected
- No indication which documents had extraction failures

*UX Improvement:*
```
‚ö†Ô∏è  EXTRACTION WARNINGS (2):

Warning 1: Monitoring Report (chunk 2/3)
Issue: LLM returned invalid response format
Impact: Date extraction incomplete for pages 15-30
Action: Re-run extraction or manually verify dates in this section

Warning 2: Baseline Report (chunk 1/1)
Issue: JSON parsing failed - possible special characters in text
Impact: Land tenure data not extracted
Action: Check document for unusual formatting, consider manual extraction
```

**Failure Mode 2: API Timeout/Rate Limit**

*Scenario:* API call times out or hits rate limit during batch extraction.

*Current Handling:* (`llm_extractors.py:109-183`)
```python
async def _call_api_with_retry(
    self, api_call, max_retries=3, initial_delay=1.0, max_delay=32.0
):
    # Exponential backoff with jitter for:
    # - RateLimitError (429)
    # - InternalServerError (500+)
    # - APIConnectionError
    # - APITimeoutError
```

*UX Impact:*
- Extraction takes longer than expected (retries)
- User has no visibility into retry process
- If all retries fail, entire requirement fails

*UX Improvement:*
```
Processing REQ-007: Project start dates...
[====================] 100% (7/7 documents)

Processing REQ-015: Baseline measurements...
[===========>        ] 55% (4/7 documents)
‚ö†Ô∏è  API rate limit hit. Retrying in 2.3s...
[=============>      ] 65% (4/7 documents, 1 retry)
‚úÖ Retry successful
[====================] 100% (7/7 documents)

Summary:
- Total API calls: 47
- Retries needed: 3
- Failed after retries: 0
- Total time: 2m 15s
```

**Failure Mode 3: Low Confidence Extraction**

*Scenario:* LLM extracts data but assigns low confidence (<0.5).

*Current Handling:* No special handling - low confidence snippets included in results.

*UX Impact:*
- User must manually review low-confidence extractions
- No guidance on whether to trust low-confidence data
- Risk of accepting false positives or rejecting true positives

*UX Improvement:*
```
REQ-015: Baseline soil carbon measurements
Status: Partial ‚ö†Ô∏è
Confidence: 0.58 (MODERATE - manual review recommended)

Evidence found (3 snippets):

1. "Soil sampling methodology described" - monitoring_plan.pdf p.8
   Confidence: 0.82 ‚≠ê‚≠ê‚≠ê‚≠ê‚ö™ STRONG
   [Auto-accepted] [Review]

2. "Baseline established in 2022" - project_summary.pdf p.3
   Confidence: 0.45 ‚≠ê‚≠ê‚ö™‚ö™‚ö™ WEAK - REVIEW REQUIRED
   [Accept] [Reject] [Edit]

   Why low confidence?
   - Mentions baseline but no specific measurements
   - No lab analysis or SOC values provided
   - Context suggests methodology only, not results

3. "Carbon sequestration potential estimated" - proposal.pdf p.12
   Confidence: 0.28 ‚≠ê‚ö™‚ö™‚ö™‚ö™ VERY WEAK - likely false positive
   [Auto-rejected] [Override]

   Why very low confidence?
   - Discusses future potential, not baseline measurements
   - Estimations are not the same as measured data
```

**Failure Mode 4: Conflicting Extractions**

*Scenario:* LLM extracts different values for same field from different documents.

*Current Handling:* Deduplication keeps highest confidence value.
```python
# evidence_tools.py:325-332
if not all_snippets:
    status = "missing"
elif all_snippets and all_snippets[0].confidence > 0.8:
    status = "covered"
elif all_snippets:
    status = "partial"
```

*UX Impact:*
- Conflicting data hidden from user
- No indication that documents disagree
- User doesn't know which value to trust

*UX Improvement:*
```
REQ-007: Project start date
Status: Flagged üö© - CONFLICT DETECTED

‚ö†Ô∏è  CONFLICTING EVIDENCE FOUND:

Value 1: "January 1, 2022"
  Source: Project Plan, Section 1.2, Page 4
  Confidence: 0.95 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

Value 2: "March 15, 2022"
  Source: Monitoring Report, Section 1.1, Page 2
  Confidence: 0.88 ‚≠ê‚≠ê‚≠ê‚≠ê‚ö™

Action Required:
[ ] Accept Value 1 (Project Plan)
[ ] Accept Value 2 (Monitoring Report)
[ ] Manual verification needed

Note: Project Plan is typically the authoritative source for start dates.
Consider selecting Value 1 unless you have reason to believe otherwise.
```

### B. Cost Tracking UX

**How is extraction cost communicated to users?**

**Current Cost Tracking:** (`cost_tracker.py`)

The system tracks:
- Per-call costs (input tokens, output tokens, cache read/write)
- Session totals
- Cache hit rates
- Breakdown by extractor type (date, tenure, project_id)

**Cost Display:** (`cost_tracker.py:236-271`)
```python
def print_summary(self):
    """Print formatted cost summary to console."""
    print(f"COST SUMMARY - Session: {self.session_id}")
    print(f"Total API Calls:     {summary.total_api_calls}")
    print(f"Cache Hit Rate:      {summary.cache_hit_rate:.1%}")
    print(f"Total:             ${summary.total_cost_usd:.4f}")
```

**Cost UX Strengths:**
- Comprehensive tracking (nothing missed)
- Transparent pricing (shows per-token costs)
- Cache effectiveness visible

**Cost UX Weaknesses:**

1. **No Upfront Cost Estimate**: User doesn't know cost before starting extraction
2. **No Budget Controls**: Can't set cost limits or warnings
3. **No Cost-Benefit Guidance**: Unclear if spending $2.50 for 90% confidence is worth it vs. $0.30 for 75%
4. **Post-Hoc Display**: Cost shown after work is done (too late to adjust)

**Cost UX Improvements:**

1. **Pre-Extraction Cost Estimate**
   ```
   /evidence-extraction --session botany-farm-2023

   üìä Extraction Plan:
   - Documents to process: 7
   - Total pages: 142
   - Estimated tokens: ~450,000
   - Estimated API calls: 23

   üí∞ Estimated Cost:
   - Input tokens:  $1.35 (450K √ó $3.00/M)
   - Output tokens: $0.45 (30K √ó $15.00/M)
   - Cache writes:  $0.28 (75K √ó $3.75/M - first run)
   - TOTAL: ~$2.08

   üí° Cost Optimization:
   - Cache enabled: Will save 90% on re-runs (~$0.20 per re-run)
   - Chunking enabled: Large docs will be split efficiently
   - Parallel processing: Will complete in ~2 minutes

   Proceed with extraction? [Y/n]
   ```

2. **Real-Time Cost Tracking During Extraction**
   ```
   Extracting evidence...
   [==========>         ] 50% complete

   Cost so far: $1.12 / ~$2.08 estimated
   Cache hits: 12/23 calls (52% hit rate)
   Time remaining: ~1m 15s

   [Press 'p' to pause, 'q' to quit and save progress]
   ```

3. **Cost-Benefit Decision Points**
   ```
   Evidence Extraction Complete!

   Current Coverage: 78% (18/23 requirements)
   Current Cost: $2.05

   ü§î Re-run Extraction?

   Option 1: Accept current results
   - Pro: No additional cost
   - Con: 2 requirements still missing

   Option 2: Re-run with larger context windows (+$0.85)
   - Pro: Might find evidence for missing requirements
   - Con: Higher cost, no guarantee of improvement

   Option 3: Re-run missing requirements only (+$0.18)
   - Pro: Targeted, cost-effective
   - Con: Only processes 2 requirements

   Recommendation: Option 3 (best cost/benefit)
   Choose [1/2/3]:
   ```

4. **Cost Attribution by Requirement**
   ```
   üìä Cost Breakdown by Requirement:

   REQ-007: Project dates
   - API calls: 2
   - Tokens: 18,500 input + 850 output
   - Cost: $0.07
   - Result: ‚úÖ Covered (0.95 confidence)
   - ROI: Excellent (high confidence for low cost)

   REQ-015: Baseline measurements
   - API calls: 5
   - Tokens: 65,000 input + 2,200 output
   - Cost: $0.23
   - Result: ‚ö†Ô∏è  Partial (0.58 confidence)
   - ROI: Poor (high cost for uncertain result)
   - Suggestion: Consider manual verification instead of re-running
   ```

5. **Budget Controls**
   ```
   /evidence-extraction --max-cost 5.00 --pause-at 4.00

   Extraction will:
   - Pause at $4.00 to ask for confirmation
   - Stop automatically at $5.00 (hard limit)
   - Save progress at each pause point

   This prevents runaway costs on large document sets.
   ```

### C. Cache Hit/Miss Transparency

**How does caching affect the user experience?**

**Current Caching Implementation:** (`cache.py`)

- File-based cache with TTL (time-to-live)
- Namespace separation (PDF extraction, GIS metadata, etc.)
- SHA256 key hashing for cache lookups
- Automatic cache expiration and cleanup

**Cache Behavior:**
```python
# cache.py:58-83
def get(self, key: str, default: Any = None) -> Any:
    """Get value from cache."""
    if not settings.enable_caching:
        return default  # Cache disabled

    if cache_data.get("ttl"):
        expires_at = cache_data["cached_at"] + cache_data["ttl"]
        if time.time() > expires_at:
            cache_path.unlink()  # Expired, delete
            return default

    return cache_data["value"]
```

**Cache UX Strengths:**
- Completely transparent (user doesn't need to manage it)
- Significant cost savings on re-runs (90% reduction via prompt caching)
- Automatic expiration prevents stale data

**Cache UX Weaknesses:**

1. **No Visibility**: User doesn't know when cache is hit/missed
2. **No Control**: Can't clear cache or disable for specific operations
3. **Stale Data Risk**: User doesn't know when cache was created
4. **No Cache Performance Metrics**: Unknown how much time/cost saved

**Cache Transparency Improvements:**

1. **Cache Status Indicators**
   ```
   Processing REQ-007: Project dates
   [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100%
   ‚úÖ Extracted 3 dates (2 documents)
   üíæ Cache: MISS - Results cached for future runs

   Processing REQ-008: Crediting period
   [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100%
   ‚úÖ Extracted 2 dates (2 documents)
   üíæ Cache: HIT - Loaded from cache (cached 5 minutes ago)
   ```

2. **Cache Statistics Summary**
   ```
   üìä Extraction Summary:

   Coverage: 78% (18/23 requirements)
   Cost: $0.42 (saved $1.86 via caching)

   Cache Performance:
   - Hits: 18/23 calls (78% hit rate) üíæ‚úÖ
   - Misses: 5/23 calls (22%)
   - Time saved: ~1m 45s
   - Cost saved: $1.86 (82% reduction)

   Cache freshness:
   - Created: 2025-11-14 10:30 AM (15 minutes ago)
   - Expires: 2025-11-14 12:30 PM (1h 45m remaining)
   ```

3. **Cache Management Commands**
   ```
   # Clear cache for specific session
   /cache clear --session botany-farm-2023
   ‚úÖ Cleared 47 cached entries

   # Clear all evidence extraction caches
   /cache clear --namespace evidence_extraction
   ‚úÖ Cleared 152 cached entries across all sessions

   # View cache status
   /cache status

   Cache Statistics:
   - Total entries: 234
   - Total size: 12.4 MB
   - Hit rate (last 24h): 67%
   - Oldest entry: 14 days ago
   - Expired entries: 23 (auto-cleanup pending)

   # Force fresh extraction (ignore cache)
   /evidence-extraction --no-cache
   ‚ö†Ô∏è  Cache disabled - extraction will take longer and cost more
   ```

4. **Cache Staleness Warnings**
   ```
   ‚ö†Ô∏è  CACHE STALENESS DETECTED

   Some cached results are older than your documents:

   REQ-007: Project dates
   - Cache created: 2025-11-10 9:00 AM
   - Document modified: 2025-11-12 2:30 PM (2 days AFTER cache)
   - Risk: Extracted data may be outdated
   - Recommendation: Re-run extraction to get fresh data

   Options:
   1. Use cached results anyway (faster, might be outdated)
   2. Re-extract this requirement only ($0.07)
   3. Re-extract all potentially stale requirements ($0.35)

   Choose [1/2/3]:
   ```

5. **Cache-Based Cost Predictions**
   ```
   /evidence-extraction --estimate

   üí∞ Cost Estimate:

   Fresh Extraction (no cache):
   - 23 requirements √ó ~$0.09 avg = ~$2.07
   - Time: ~3 minutes

   With Current Cache (78% hit rate):
   - 5 new extractions √ó $0.09 = $0.45
   - 18 cached results = $0.00
   - TOTAL: ~$0.45 (78% savings)
   - Time: ~45 seconds

   üí° Tip: Cache will expire in 1h 45m. Run now to benefit from savings.
   ```

### D. Confidence Score Interpretation

**How are confidence scores presented and understood?**

**Current Confidence Scoring:**

**Keyword-Based (evidence_tools.py):**
```python
# Snippet confidence based on keyword density
keywords_in_snippet = sum(1 for kw in keywords if kw.lower() in snippet_lower)
confidence = min(keywords_in_snippet / len(keywords), 1.0)

# Overall requirement confidence = highest snippet confidence
confidence = max(s.confidence for s in all_snippets)
```

**LLM-Based (llm_extractors.py):**
```python
# LLM assigns confidence based on:
# 1.0 = explicit, unambiguous statement
# 0.8 = inferred from context
# 0.5 = ambiguous or unclear
```

**Status Thresholds:**
```python
if all_snippets and all_snippets[0].confidence > 0.8:
    status = "covered"
elif all_snippets:
    status = "partial"
else:
    status = "missing"
```

**Confidence Interpretation Challenges:**

1. **Threshold Ambiguity**: Why is 0.8 the cutoff for "covered"?
2. **No Contextual Meaning**: Is 0.72 good or bad? Depends on requirement criticality.
3. **Mixed Signals**: Requirement with 3 snippets (0.9, 0.6, 0.4) ‚Üí confidence 0.9 ‚Üí "covered" (but 2/3 snippets are weak)
4. **No Calibration Feedback**: User can't tell if confidence scores are well-calibrated to reality

**Confidence UX Improvements:**

1. **Confidence Bands with Descriptions**
   ```
   Confidence Levels:

   0.90 - 1.00 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê VERY STRONG
   - Explicit statement of requirement
   - Multiple document confirmation
   - No ambiguity detected
   - Action: Accept with minimal review

   0.75 - 0.89 ‚≠ê‚≠ê‚≠ê‚≠ê‚ö™ STRONG
   - Clear evidence present
   - Minor ambiguity or single source
   - Action: Quick verification recommended

   0.60 - 0.74 ‚≠ê‚≠ê‚≠ê‚ö™‚ö™ MODERATE
   - Evidence found but incomplete
   - Requires interpretation
   - Action: Manual review required

   0.40 - 0.59 ‚≠ê‚≠ê‚ö™‚ö™‚ö™ WEAK
   - Ambiguous or circumstantial evidence
   - High risk of false positive
   - Action: Verify against source documents

   0.00 - 0.39 ‚≠ê‚ö™‚ö™‚ö™‚ö™ VERY WEAK
   - Likely false positive or inference
   - Little direct support for requirement
   - Action: Reject or request additional documents
   ```

2. **Confidence Explanation (LLM Reasoning)**
   ```
   REQ-015: Baseline soil carbon measurements
   Confidence: 0.58 ‚≠ê‚≠ê‚≠ê‚ö™‚ö™ MODERATE

   Why this confidence level?
   ‚úÖ Found: Sampling methodology described in monitoring plan
   ‚úÖ Found: Sampling dates and locations specified
   ‚ö†Ô∏è  Unclear: No explicit measurement results found
   ‚ö†Ô∏è  Unclear: Lab analysis reports not referenced
   ‚ùå Missing: Actual SOC values not stated

   Confidence breakdown:
   - Methodology evidence: 0.85 (strong)
   - Measurement evidence: 0.30 (weak)
   - Overall: 0.58 (weighted average, methodology partial credit)

   To improve confidence:
   - Find lab analysis reports or measurement tables
   - Look for appendices with raw data
   ```

3. **Multi-Snippet Aggregation Transparency**
   ```
   REQ-007: Project start date
   Overall Confidence: 0.95 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê VERY STRONG

   Evidence from 3 snippets:
   1. "Project commenced January 1, 2022" - project_plan.pdf p.4
      Confidence: 0.98 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

   2. "Start date: 01/01/2022" - monitoring_report.pdf p.2
      Confidence: 0.95 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

   3. "Jan 2022 baseline established" - baseline_report.pdf p.1
      Confidence: 0.88 ‚≠ê‚≠ê‚≠ê‚≠ê‚ö™

   Aggregation method: Highest confidence (0.98)
   Cross-verification: ‚úÖ All 3 sources agree on date

   Confidence is VERY STRONG because:
   - Multiple independent confirmations
   - Consistent date across documents
   - Explicit statement format
   ```

4. **Confidence Calibration Report**
   ```
   üìä Confidence Calibration Analysis

   After human review of 23 requirements:

   Accuracy by Confidence Band:
   - Very Strong (0.9-1.0): 100% correct (15/15) ‚úÖ
   - Strong (0.75-0.89): 100% correct (3/3) ‚úÖ
   - Moderate (0.6-0.74): 67% correct (2/3) ‚ö†Ô∏è
   - Weak (0.4-0.59): 50% correct (1/2) ‚ö†Ô∏è
   - Very Weak (<0.4): 0% correct (0/0) N/A

   Model Calibration: EXCELLENT
   - Confidence scores accurately reflect true accuracy
   - High confidence = trustworthy
   - Low confidence = needs review (as expected)

   Recommendations:
   - Trust Very Strong/Strong evidence (>0.75)
   - Always review Moderate/Weak evidence (<0.75)
   - Current threshold (0.8 for "covered") is well-calibrated
   ```

5. **Risk-Adjusted Confidence**
   ```
   REQ-015: Baseline soil carbon measurements
   Statistical Confidence: 0.72 ‚≠ê‚≠ê‚≠ê‚ö™‚ö™
   Requirement Criticality: üî¥ CRITICAL (required for crediting)
   Risk-Adjusted Action: ‚ö†Ô∏è  MANUAL VERIFICATION REQUIRED

   Explanation:
   While statistical confidence is "moderate," this requirement is
   CRITICAL for carbon credit issuance. Even moderate confidence is
   not sufficient to approve without human verification.

   For critical requirements, we require confidence >0.85 for auto-approval.

   REQ-009: Project contact information
   Statistical Confidence: 0.68 ‚≠ê‚≠ê‚≠ê‚ö™‚ö™
   Requirement Criticality: üü° MEDIUM (administrative)
   Risk-Adjusted Action: ‚úÖ ACCEPTABLE (low risk if slightly incomplete)

   Explanation:
   For administrative requirements, moderate confidence is acceptable.
   Minor errors can be corrected later without compliance impact.
   ```

### E. Manual Evidence Addition Workflows

**How can Becca add evidence that the agent missed?**

**Current Capability:** None. No manual evidence addition supported.

**Problem Scenarios:**

1. **Evidence in Non-Text Formats**: Scanned images, handwritten notes, tables
2. **Evidence Requiring Domain Knowledge**: "The project follows the standard protocol" (reviewer knows this implies X, Y, Z)
3. **Evidence from External Sources**: Email confirmations, phone conversations, site visits
4. **Evidence from Multiple Documents**: Requirement satisfied by combination of 3 paragraphs from different docs

**Manual Addition Workflow Design:**

**Workflow 1: Add Evidence Snippet from Discovered Document**
```
Command: /add-evidence REQ-015

Step 1: Select source document
Available documents:
1. baseline_report.pdf
2. monitoring_plan.pdf
3. project_plan.pdf
4. [Upload new document]

Choose [1-4]: 1

Step 2: Specify location
baseline_report.pdf opened in viewer.
Navigate to evidence location:
- Page number: 12
- Section (optional): "3.2 Baseline Measurements"

Step 3: Extract or describe evidence
Option A: Highlight text in viewer (OCR extraction)
Option B: Type/paste evidence manually

> "Baseline SOC measured at 45.2 Mg C/ha across all project parcels"

Step 4: Set confidence and notes
Confidence (0.0-1.0): 1.0
Reasoning: "Manual extraction from Table 3, verified against lab reports"

Step 5: Confirm
‚úÖ Evidence added:
   REQ-015: Baseline soil carbon measurements
   Status updated: missing ‚Üí covered
   Confidence: 1.0 (manual verification)

Would you like to add more evidence? [y/N]
```

**Workflow 2: Add Evidence from External Source**
```
Command: /add-evidence REQ-018 --external

Step 1: Describe external source
Source type:
1. Email communication
2. Phone conversation
3. Site visit observation
4. Other

Choose [1-4]: 1

Step 2: Provide source details
Sender: john.doe@ecometric.com
Date: 2025-11-10
Subject: "Re: Additionality documentation"

Step 3: Describe evidence
Evidence text:
> "Project demonstrates additionality per Section 4.2 of methodology.
  Land was previously degraded pasture, not under any carbon program.
  Baseline scenario would be continued conventional grazing."

Step 4: Attach supporting documents (optional)
Would you like to attach email .eml file? [y/N]: y
Upload file: additionality_email.eml
‚úÖ Uploaded and linked to evidence

Step 5: Set confidence
Confidence: 0.90
Reasoning: "Direct confirmation from project developer, supported by email"

‚úÖ Evidence added:
   REQ-018: Additionality demonstration
   Status updated: missing ‚Üí covered
   Confidence: 0.90 (manual - external source)
   Note: Evidence from email communication (attached)
```

**Workflow 3: Composite Evidence (Multiple Sources)**
```
Command: /add-composite-evidence REQ-015

REQ-015 requires multiple pieces of information:
- Baseline SOC measurements
- Sampling methodology
- Lab analysis certification

Current status: Partial (methodology found, measurements missing)

Step 1: Add first evidence component
Component: Baseline SOC measurements
Document: lab_results.xlsx (upload new)
Location: Sheet "Summary", Cell B12
Value: "45.2 Mg C/ha"
Confidence: 1.0

Step 2: Add second component
Component: Lab certification
Document: lab_certificate.pdf (upload new)
Location: Page 1
Value: "ISO 17025 accredited"
Confidence: 1.0

Step 3: Review composite evidence
REQ-015: Baseline soil carbon measurements
Composite evidence (3 parts):
‚úÖ Methodology described (auto-extracted, confidence 0.85)
‚úÖ SOC measurements documented (manual, confidence 1.0)
‚úÖ Lab certification verified (manual, confidence 1.0)

Overall confidence: 0.95 (weighted average, manual components prioritized)
Status: Covered ‚úÖ

Confirm and save? [Y/n]
```

**Workflow 4: Bulk Manual Verification**
```
Command: /batch-verify-evidence

Review all partial/missing requirements:

[1/5] REQ-015: Baseline soil carbon measurements (Partial, 0.58)
      Found: "Sampling methodology described"
      Missing: Actual measurement values

      Action:
      [ ] Found in document (specify location)
      [ ] Found in external source
      [X] Truly missing - request from developer

[2/5] REQ-018: Additionality demonstration (Missing, 0.00)
      No evidence found.

      Action:
      [X] Found in document (specify location)
      [ ] Found in external source
      [ ] Truly missing

      > Document: additionality_memo.pdf
      > Page: 3
      > Evidence: "Project implemented on degraded pasture..."
      > Confidence: 0.95

[Ctrl+S to save progress, Ctrl+Q to finish]

Summary of changes:
- REQ-015: Marked as "missing" with developer request note
- REQ-018: Evidence added, status ‚Üí covered
- REQ-020: Evidence added, status ‚Üí covered
- REQ-021: Marked as "missing" with developer request note

‚úÖ Batch verification complete. 2 requirements updated.
```

### F. Partial Extraction Recovery

**What happens when extraction fails mid-process?**

**Failure Scenarios:**

1. **API Timeout Mid-Batch**: Processing requirement 15/23, API times out, loses progress
2. **User Interruption**: User hits Ctrl+C during extraction
3. **System Crash**: Power loss, network failure, OOM error
4. **Partial Document Processing**: Some documents extracted, others failed

**Current Behavior:** (`evidence_extraction.py:82-86`)
```python
results = await evidence_tools.extract_all_evidence(session_id)

# extract_all_evidence processes ALL requirements sequentially
# If failure occurs, entire extraction fails - no partial save
```

**Problem:** No checkpoint/resume capability. User must re-run entire extraction.

**Partial Recovery Design:**

**Strategy 1: Checkpoint Every N Requirements**
```
Extracting evidence for 23 requirements...

[=====               ] 25% (5/23) - Checkpoint saved
[==========          ] 50% (12/23) - Checkpoint saved
[===============     ] 75% (18/23) - Checkpoint saved

‚ö†Ô∏è  API timeout on REQ-019

Recovery options:
1. Resume from last checkpoint (requirement 18/23)
2. Retry failed requirement only (REQ-019)
3. Restart entire extraction

Choose [1/2/3]: 1

Resuming from checkpoint...
[===================>] 95% (22/23)
[====================] 100% (23/23) ‚úÖ

Extraction complete with recovery:
- Original run: 18/23 requirements (checkpoint saved)
- Resumed run: 5/23 requirements
- Total time: 2m 15s (saved 1m 30s via checkpoint)
```

**Strategy 2: Per-Requirement State Tracking**
```
Evidence extraction state saved to: evidence_state.json

{
  "session_id": "botany-farm-2023",
  "started_at": "2025-11-14T10:30:00Z",
  "requirements": {
    "REQ-001": {"status": "completed", "extracted_at": "..."},
    "REQ-002": {"status": "completed", "extracted_at": "..."},
    ...
    "REQ-018": {"status": "in_progress", "started_at": "..."},
    "REQ-019": {"status": "pending"},
    "REQ-020": {"status": "pending"}
  }
}

When resuming:
‚úÖ Skip REQ-001 through REQ-017 (already completed)
‚è≠Ô∏è  Resume REQ-018 (in progress)
üîÑ Process REQ-019 through REQ-023 (pending)
```

**Strategy 3: Graceful Degradation**
```
Extracting evidence...
[===============>    ] 75% (17/23)

‚ö†Ô∏è  API timeout on REQ-018 (retry 3/3 failed)

Options:
1. Mark REQ-018 as "flagged" and continue with remaining 6 requirements
2. Pause here and save progress, retry later
3. Abort and discard all progress

Recommendation: Option 1 (graceful degradation)
You can retry REQ-018 individually later.

Choose [1/2/3]: 1

‚úÖ Extraction completed with warnings:
- Covered: 15/23 requirements
- Partial: 4/23 requirements
- Missing: 3/23 requirements
- Flagged: 1/23 requirements (REQ-018 - extraction failed)

Flagged requirement can be retried:
/evidence-extraction --requirements REQ-018
```

**Strategy 4: User-Initiated Pause/Resume**
```
Extracting evidence...
[=========>          ] 45% (10/23)

[User presses 'p' to pause]

‚è∏Ô∏è  Extraction paused

Progress saved:
- Completed: 10/23 requirements
- Current: REQ-011 (50% processed)
- Remaining: 13 requirements
- Time elapsed: 1m 15s
- Estimated remaining: 1m 30s

Options:
[R]esume extraction
[S]ave and exit (can resume later)
[A]bort (discard progress)

Choose [R/S/A]: S

‚úÖ Progress saved to: evidence_extraction_checkpoint.json

To resume later:
/evidence-extraction --resume
```

**Strategy 5: Idempotent Re-runs with Smart Merge**
```
/evidence-extraction

Detected existing evidence from previous run:
- Extraction date: 2025-11-14 9:00 AM (1 hour 30 minutes ago)
- Coverage: 15/23 requirements completed
- Status: Incomplete (8 requirements pending)

Options:
1. Resume incomplete extraction (extract only 8 pending requirements)
2. Re-extract all 23 requirements (overwrite previous results)
3. Re-extract failed/flagged requirements only (1 requirement)
4. Merge with previous results (keep existing, add new)

Choose [1/2/3/4]: 1

Resuming extraction for 8 pending requirements...
[====================] 100% (8/8) ‚úÖ

Merging with previous results...
‚úÖ Complete extraction:
- From previous run: 15 requirements
- From current run: 8 requirements
- Total coverage: 23/23 requirements (100%)
```

---

## 4. UX Principles Applied

### Principle 1: Progressive Disclosure
**Application:**
- Show summary statistics first (overall coverage %)
- Expand to requirement-by-requirement details on request
- Deepest level: individual snippets with full context

**Implementation:**
- Collapsible sections in UI
- Keyboard shortcuts to jump between levels
- Summary ‚Üí Covered ‚Üí Partial ‚Üí Missing navigation flow

### Principle 2: Clear Feedback
**Application:**
- Every extraction action provides immediate feedback
- Confidence scores explained, not just numbers
- Cache hits/misses visible
- Cost tracking transparent

**Implementation:**
- Real-time progress indicators
- Confidence bands with descriptions
- Cache status icons
- Cost estimates before and after

### Principle 3: Error Prevention
**Application:**
- Pre-extraction cost estimates prevent surprises
- Confidence thresholds prevent accepting bad data
- Manual verification prompts for critical requirements
- Conflict detection before committing results

**Implementation:**
- "Proceed with extraction?" confirmation with cost estimate
- Auto-flagging of low-confidence extractions
- Required review for requirements below critical threshold
- Conflict resolution workflows

### Principle 4: User Control
**Application:**
- User can adjust confidence thresholds
- User can manually add/edit evidence
- User can pause/resume extraction
- User can clear cache or force fresh extraction

**Implementation:**
- `/add-evidence` command for manual addition
- `/evidence-extraction --no-cache` for fresh runs
- Pause/resume via keyboard shortcuts
- Confidence threshold configuration

### Principle 5: Consistency
**Application:**
- Confidence scores use same visual language across stages
- Status indicators (‚úÖ ‚ö†Ô∏è ‚ùå üö©) consistent throughout
- Cost formatting consistent ($X.XX format)
- Document citation format standardized

**Implementation:**
- Shared confidence band definitions
- Consistent icon set
- Standard cost display formatting
- Citation template: "document_name, page X, section Y"

---

## 5. Critical UX Questions Answered

### Q1: How does LLM extraction handle ambiguity?
**Answer:**
- **Confidence Scoring**: LLM assigns 0.5 confidence for ambiguous extractions
- **Reasoning Field**: Every extraction includes explanation of why confidence was assigned
- **Conflict Detection**: Multiple conflicting values trigger flag for human review
- **Graceful Degradation**: Invalid responses logged but don't crash entire extraction

**UX Improvement Needed:**
Expose LLM reasoning to user in UI, not just logs. Show "why low confidence" explanations.

### Q2: What happens when evidence is weak or missing?
**Answer:**
- **Status Categorization**: Automatically marked as "partial" (weak) or "missing" (none)
- **Visual Distinction**: Different icons (‚ö†Ô∏è for partial, ‚ùå for missing)
- **Actionable Guidance**: Future improvement to suggest where to look

**UX Improvement Needed:**
Add "what to do next" recommendations for missing evidence (check appendices, contact developer, etc.).

### Q3: How does caching affect the experience?
**Answer:**
- **Performance**: 90% faster on re-runs (cache hits)
- **Cost**: 90% cheaper via prompt caching (cache read tokens = 10% of input tokens)
- **Transparency**: Currently invisible to user

**UX Improvement Needed:**
Surface cache statistics (hit rate, savings, freshness) in UI.

### Q4: What if API calls fail mid-extraction?
**Answer:**
- **Retry Logic**: Exponential backoff with jitter (up to 3 retries)
- **Current Limitation**: No checkpoint/resume (must restart entire extraction)
- **Partial Failure Handling**: Invalid JSON logged but extraction continues for other requirements

**UX Improvement Needed:**
Implement checkpoint/resume capability to save progress every N requirements.

### Q5: How does Becca review extracted evidence?
**Answer:**
- **Current**: Text-based output in console (passive reading)
- **Limitation**: No interactive review interface
- **Manual Process**: Becca must cross-reference printed output with documents manually

**UX Improvement Needed:**
- Interactive review interface (web UI or TUI)
- Side-by-side document viewer with evidence highlighting
- Accept/reject/edit controls for each snippet
- Batch verification workflows

### Q6: What's the cost/time tradeoff communication?
**Answer:**
- **Current**: Cost displayed after extraction completes
- **Limitation**: No upfront estimate or budget controls
- **Time Estimate**: Not provided

**UX Improvement Needed:**
- Pre-extraction cost/time estimates
- Real-time cost tracking during extraction
- Budget limit controls (pause at $X)
- Cost-benefit analysis for re-runs

### Q7: How are confidence scores presented?
**Answer:**
- **Format**: Numerical (0.0 - 1.0) with 2 decimal places
- **Context**: Shown per snippet and aggregated per requirement
- **Interpretation**: No visual aids or contextual guidance

**UX Improvement Needed:**
- Visual confidence indicators (star ratings, color coding)
- Confidence band descriptions (strong/moderate/weak)
- Reasoning explanations from LLM
- Calibration reports showing accuracy by confidence band

### Q8: What about false positives/negatives?
**Answer:**
- **False Positives**: Low-confidence extractions flagged for review
- **False Negatives**: Missing evidence prompts manual verification
- **Learning**: No feedback loop (system doesn't learn from mistakes)

**UX Improvement Needed:**
- Explicit accept/reject controls for each evidence snippet
- Feedback loop to improve future extractions
- False positive/negative tracking and reporting
- Model retraining based on corrections

---

## 6. Failure Mode Matrix

| Failure Mode | Current Handling | User Impact | Severity | UX Improvement |
|-------------|-----------------|-------------|----------|----------------|
| **Invalid JSON from LLM** | Log error, return empty list | Silent failure, missing evidence | Medium | Show extraction warning with affected requirements |
| **API timeout** | Retry 3x with exponential backoff | Slower extraction, user waits | Low | Show retry status, allow pause/resume |
| **Rate limit hit** | Retry with backoff | Slower extraction | Low | Show rate limit warning, suggest scheduling |
| **Network failure** | Retry, then fail | Lost progress, must restart | High | Checkpoint/resume capability |
| **User interruption (Ctrl+C)** | Abort, lose all progress | Wasted time/cost | High | Save state on interrupt, allow resume |
| **Low confidence extraction** | Include in results | User must manually review | Low | Auto-flag for review, explain why low |
| **Conflicting extractions** | Keep highest confidence | Conflicts hidden from user | Medium | Surface conflicts, require resolution |
| **Missing evidence** | Mark as "missing" | No actionable guidance | Low | Suggest where to look (appendices, etc.) |
| **Partial evidence** | Mark as "partial" | Unclear what's missing | Low | Explain what was found and what's missing |
| **Cache staleness** | Silently use old data | Risk of outdated extractions | Medium | Warn if cache older than documents |
| **Cost overrun** | No limit, charge full amount | Unexpected high costs | Medium | Budget controls, pause at threshold |
| **Malformed document** | Extraction fails or partial | Incomplete results | Low | Flag document as problematic, suggest re-upload |

---

## 7. Recommendations Summary

### High Priority (Implement for MVP)

1. **Confidence Interpretation Aids**
   - Visual indicators (star ratings, color bands)
   - Descriptive labels (STRONG/MODERATE/WEAK)
   - Reasoning explanations for low confidence

2. **Cost Transparency**
   - Pre-extraction cost estimates
   - Real-time cost tracking during extraction
   - Cache savings displayed

3. **Partial/Missing Evidence Guidance**
   - "What's missing" explanations
   - Actionable next steps ("check appendices", "contact developer")
   - Priority indicators (critical vs. optional)

4. **Checkpoint/Resume Capability**
   - Save progress every 5 requirements
   - Allow resume after interruption
   - Graceful degradation (continue despite failures)

5. **Cache Visibility**
   - Cache hit/miss indicators per requirement
   - Freshness warnings (cache older than documents)
   - Manual cache management commands

### Medium Priority (Post-MVP Enhancements)

6. **Manual Evidence Addition**
   - `/add-evidence` command for supplementing extractions
   - Document viewer integration
   - External source evidence (emails, phone calls)

7. **Conflict Detection and Resolution**
   - Surface conflicting extractions
   - Require user resolution
   - Authoritative source guidance

8. **Interactive Review Interface**
   - Web UI or TUI for evidence review
   - Accept/reject controls per snippet
   - Side-by-side document viewer

9. **Batch Verification Workflows**
   - Bulk review of partial/missing requirements
   - Keyboard-driven navigation
   - Quick accept/reject/edit actions

10. **Learning Feedback Loop**
    - Track accept/reject decisions
    - Improve confidence calibration
    - Retrain extraction prompts

### Low Priority (Future Optimizations)

11. **Advanced Filtering and Sorting**
    - Filter by confidence range
    - Sort by priority/category
    - Group by document source

12. **Comparative Benchmarking**
    - "Typical coverage for this methodology"
    - Historical project comparison
    - Best practice recommendations

13. **Risk-Adjusted Confidence**
    - Critical requirements require higher confidence
    - Administrative requirements accept lower confidence
    - Dynamic thresholds based on requirement type

14. **Cost Optimization Suggestions**
    - "Re-run missing only" vs. "re-run all"
    - Batch processing recommendations
    - Cache warming strategies

15. **Multi-Session Comparison**
    - Compare extractions across project versions
    - Diff view for changed evidence
    - Reversion capability

---

## 8. Metrics for Success

### User Satisfaction Metrics
- **Confidence in Results**: % of users who trust agent extractions without full document review
- **Time Savings**: Minutes saved vs. manual evidence extraction
- **Ease of Review**: User rating of evidence review interface (1-5 scale)
- **Error Detection**: % of false positives/negatives caught before approval

### System Performance Metrics
- **Extraction Accuracy**: % of evidence correctly mapped to requirements
- **Confidence Calibration**: Correlation between confidence scores and human agreement
- **Cache Hit Rate**: % of extractions served from cache (target: >70%)
- **Cost Efficiency**: $/requirement extracted (target: <$0.15/requirement)
- **Recovery Success**: % of interrupted extractions successfully resumed

### Operational Metrics
- **Coverage Percentage**: Average % of requirements with evidence (target: >85%)
- **False Positive Rate**: % of "covered" requirements that are actually missing evidence (target: <5%)
- **False Negative Rate**: % of "missing" requirements that actually have evidence (target: <10%)
- **Manual Intervention Rate**: % of requirements requiring manual evidence addition (target: <20%)
- **Extraction Failure Rate**: % of extractions that fail and cannot recover (target: <2%)

---

## Appendix A: Evidence Data Model

```python
class EvidenceSnippet:
    text: str                    # Extracted snippet (~100 words context)
    document_id: str             # Source document identifier
    document_name: str           # Human-readable document name
    page: int | None            # Page number (1-indexed)
    section: str | None         # Section header
    confidence: float            # 0.0 - 1.0
    keywords_matched: list[str]  # Keywords found in this snippet

class MappedDocument:
    document_id: str             # Document identifier
    document_name: str           # Document filename
    filepath: str                # Full path to document
    relevance_score: float       # 0.0 - 1.0
    keywords_found: list[str]    # Keywords from requirement found in doc

class RequirementEvidence:
    requirement_id: str          # e.g., "REQ-015"
    requirement_text: str        # Full requirement description
    category: str                # Requirement category
    status: str                  # "covered" | "partial" | "missing" | "flagged"
    confidence: float            # Overall confidence (0.0 - 1.0)
    mapped_documents: list[MappedDocument]  # Relevant documents
    evidence_snippets: list[EvidenceSnippet]  # Extracted evidence
    notes: str | None           # Human or agent notes
    extracted_at: str           # ISO timestamp

class EvidenceExtractionResult:
    session_id: str              # Session identifier
    requirements_total: int      # Total requirements in checklist
    requirements_covered: int    # Requirements with strong evidence
    requirements_partial: int    # Requirements with weak evidence
    requirements_missing: int    # Requirements with no evidence
    requirements_flagged: int    # Requirements with extraction errors
    overall_coverage: float      # Weighted coverage (0.0 - 1.0)
    evidence: list[RequirementEvidence]  # Per-requirement evidence
    extracted_at: str           # ISO timestamp
```

---

## Appendix B: Confidence Calibration Example

**Scenario:** Soil Carbon v1.2.2 project with 23 requirements

**Initial Extraction Results:**
- 15 requirements: confidence > 0.8 ‚Üí marked "covered"
- 5 requirements: confidence 0.5-0.8 ‚Üí marked "partial"
- 3 requirements: confidence < 0.5 ‚Üí marked "missing"

**Human Review Findings:**
- Of 15 "covered": 14 confirmed correct, 1 actually partial (93% accuracy)
- Of 5 "partial": 3 confirmed partial, 2 actually covered (60% accuracy)
- Of 3 "missing": 2 confirmed missing, 1 actually partial (67% accuracy)

**Calibration Adjustments:**
- Confidence threshold for "covered" increased from 0.8 ‚Üí 0.85
- "Partial" range narrowed to 0.6-0.84 (was 0.5-0.8)
- Added "weak" category for 0.4-0.59 (auto-flagged for review)
- <0.4 treated as "very weak" (likely false positive, rejected)

**Post-Calibration Results:**
- "Covered" accuracy: 93% ‚Üí 98%
- "Partial" accuracy: 60% ‚Üí 85%
- "Missing" accuracy: 67% ‚Üí 90%
- Overall accuracy: 78% ‚Üí 92%

**Key Insight:** Confidence scores are well-calibrated when accuracy in each band matches the confidence level (e.g., 0.8 confidence should be 80% accurate).

---

*End of Stage 3 UX Analysis*
