"""Registry Review MCP Server - automated carbon credit project registry review workflows."""

import logging
import sys
from datetime import datetime

from mcp.server.fastmcp import FastMCP
from mcp.types import TextContent

from .config.settings import settings
from .models import errors as mcp_errors
from .tools import session_tools, document_tools, evidence_tools, validation_tools, report_tools, upload_tools
from .utils.tool_helpers import (
    with_error_handling,
    format_success,
    format_error,
    format_list,
    format_sessions_list,
    format_session_created,
    format_session_loaded,
    format_documents_discovered,
    format_evidence_extracted,
    ResponseBuilder,
)
from .prompts import A_initialize, B_document_discovery, C_evidence_extraction, D_cross_validation, E_report_generation, F_human_review, G_complete

# ============================================================================
# Logging Setup (CRITICAL: Must write to stderr, NOT stdout)
# ============================================================================

logging.basicConfig(
    level=settings.log_level,
    format=settings.log_format,
    stream=sys.stderr,  # CRITICAL: stderr only for MCP protocol
)
logger = logging.getLogger(__name__)

# ============================================================================
# MCP Server Initialization
# ============================================================================

mcp = FastMCP("Regen Registry Review")

logger.info("Initializing Registry Review MCP Server v2.0.0")

# ============================================================================
# Session Management Tools
# ============================================================================


@mcp.tool()
@with_error_handling("create_session")
async def create_session(
    project_name: str,
    documents_path: str,
    methodology: str = "soil-carbon-v1.2.2",
    project_id: str | None = None,
    proponent: str | None = None,
    crediting_period: str | None = None,
) -> str:
    """Create a new registry review session."""
    result = await session_tools.create_session(
        project_name=project_name,
        documents_path=documents_path,
        methodology=methodology,
        project_id=project_id,
        proponent=proponent,
        crediting_period=crediting_period,
    )
    return format_session_created(result)


@mcp.tool()
@with_error_handling("load_session")
async def load_session(session_id: str) -> str:
    """Load an existing review session."""
    session_data = await session_tools.load_session(session_id)
    return format_session_loaded(session_data)


@mcp.tool()
@with_error_handling("update_session_state")
async def update_session_state(
    session_id: str,
    status: str | None = None,
    workflow_stage: str | None = None,
    workflow_status: str | None = None,
) -> str:
    """Update session state."""
    updates: dict = {}
    if status:
        updates["status"] = status
    if workflow_stage and workflow_status:
        updates[f"workflow_progress.{workflow_stage}"] = workflow_status

    result = await session_tools.update_session_state(session_id, updates)
    return format_success("Session Updated", {"session_id": session_id, "updated": result.get('updated_at')})


@mcp.tool()
@with_error_handling("list_sessions")
async def list_sessions() -> str:
    """List all review sessions with complete field details"""
    sessions = await session_tools.list_sessions()
    return format_sessions_list(sessions)


@mcp.tool()
async def list_example_projects() -> str:
    """List example projects available in the examples directory for testing/demo purposes"""
    try:
        logger.info("Listing example projects")

        result = await session_tools.list_example_projects()

        if result["projects_found"] == 0:
            return f"âœ— {result['message']}"

        project_list = []
        for project in result["projects"]:
            project_list.append(
                f"â€¢ **{project['name']}**\n"
                f"  Path: {project['path']}\n"
                f"  Files: {project['file_count']}"
            )

        return (
            f"âœ“ {result['message']}\n\n"
            + "\n\n".join(project_list)
            + "\n\n---\n\n"
            "To initialize an example project:\n"
            "/initialize Project Name, /path/from/above"
        )

    except Exception as e:
        logger.error(f"Failed to list example projects: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


@mcp.tool()
async def start_review(
    project_name: str,
    documents_path: str,
    methodology: str = "soil-carbon-v1.2.2",
    project_id: str | None = None,
) -> str:
    """Quick-start: Create session and discover documents in one step.

    This is a convenience tool that combines create_session and discover_documents
    for a streamlined workflow.

    Args:
        project_name: Name of the project being reviewed
        documents_path: Absolute path to directory containing project documents
        methodology: Methodology identifier (default: soil-carbon-v1.2.2)
        project_id: Project ID if known (e.g., C06-4997)

    Returns:
        Combined session creation and document discovery results
    """
    try:
        logger.info(f"Starting review for project: {project_name}")

        session_result = await session_tools.create_session(
            project_name=project_name,
            documents_path=documents_path,
            methodology=methodology,
            project_id=project_id,
        )

        session_id = session_result["session_id"]
        logger.info(f"Session created: {session_id}")

        discovery_result = await document_tools.discover_documents(session_id)

        summary_lines = []
        for doc_type, count in sorted(discovery_result["classification_summary"].items()):
            summary_lines.append(f"  - {doc_type}: {count}")

        summary = "\n".join(summary_lines) if summary_lines else "  (none found)"

        return f"""âœ“ Review Started Successfully

Session ID: {session_id}
Project: {project_name}
Methodology: {methodology}
Documents Path: {documents_path}
Created: {session_result['created_at']}

Document Discovery Complete:
  Found {discovery_result['documents_found']} document(s)

Classification Summary:
{summary}

Next Steps:
  - Review the document classifications
  - Use the /document-discovery prompt for detailed results
  - Or proceed to evidence extraction (Phase 3)
"""

    except Exception as e:
        logger.error(f"Failed to start review: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


@mcp.tool()
async def delete_session(session_id: str) -> str:
    """Delete a review session and all its data.

    Args:
        session_id: Unique session identifier

    Returns:
        Confirmation message
    """
    try:
        logger.info(f"Deleting session: {session_id}")

        result = await session_tools.delete_session(session_id)

        return f"âœ“ {result['message']}"

    except mcp_errors.SessionNotFoundError:
        return f"âœ— Session not found: {session_id}"

    except Exception as e:
        logger.error(f"Failed to delete session: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


# ============================================================================
# File Upload Tools
# ============================================================================


@mcp.tool()
async def create_session_from_uploads(
    project_name: str,
    files: list[dict],
    methodology: str = "soil-carbon-v1.2.2",
    project_id: str | None = None,
    proponent: str | None = None,
    crediting_period: str | None = None,
    deduplicate: bool = True,
    force_new_session: bool = False,
) -> str:
    """Create a new registry review session from uploaded file content.

    Accepts file content in two flexible formats:
    1. Base64 format: {"filename": "doc.pdf", "content_base64": "JVBERi0..."}
    2. Path format: {"filename": "doc.pdf", "path": "/absolute/path/to/file.pdf"}

    Ideal for web applications, chat interfaces, and APIs where files are
    uploaded dynamically or stored on the server filesystem.

    Args:
        project_name: Name of the project being reviewed
        files: List of file objects. Each file must have:
            - filename OR name (required): Name of the file (e.g., "ProjectPlan.pdf")
            - content_base64 OR path (required): Either base64-encoded content or absolute file path
            - mime_type (optional): MIME type (e.g., "application/pdf")
        methodology: Methodology identifier (default: soil-carbon-v1.2.2)
        project_id: Project ID if known (e.g., C06-4997)
        proponent: Name of project proponent
        crediting_period: Crediting period description (e.g., "2022-2032")
        deduplicate: Automatically remove duplicate files by filename and content hash (default: True)
        force_new_session: Force creation of new session even if existing session detected (default: False)

    Returns:
        Success message with session details and document discovery results.
        If existing session detected, returns that session's information.

    Examples:
        # Base64 format
        files = [{"filename": "plan.pdf", "content_base64": "JVBERi0xLjQK..."}]

        # Path format (e.g., from ElizaOS uploads)
        files = [{"name": "plan.pdf", "path": "/media/uploads/agents/abc/plan.pdf"}]

        create_session_from_uploads("My Project", files)
    """
    try:
        logger.info(f"Creating session from uploads: {project_name}")

        result = await upload_tools.create_session_from_uploads(
            project_name=project_name,
            files=files,
            methodology=methodology,
            project_id=project_id,
            proponent=proponent,
            crediting_period=crediting_period,
            deduplicate=deduplicate,
            force_new_session=force_new_session,
        )

        if result.get("existing_session_detected"):
            logger.info(f"Existing session detected: {result['session_id']}")

            progress = result.get("workflow_progress", {})
            progress_text = []
            for stage, status in progress.items():
                icon = "âœ“" if status == "completed" else "â³" if status == "in_progress" else "â—‹"
                progress_text.append(f"  {icon} {stage}: {status}")
            progress_summary = "\n".join(progress_text) if progress_text else "  (No progress recorded)"

            return f"""â„¹ï¸ Existing Session Found

Session ID: {result['session_id']}
Project: {result['project_name']}
Created: {result.get('session_created', 'Unknown')}

{result.get('message', 'Returning existing session.')}

Workflow Progress:
{progress_summary}

To create a new session anyway, set force_new_session=True.
"""

        logger.info(f"Session created from uploads: {result['session_id']}")

        files_list = "\n".join(f"  - {f}" for f in result["files_saved"])

        doc_types = []
        for doc_type, count in sorted(result["documents_by_type"].items()):
            doc_types.append(f"  - {doc_type}: {count}")
        doc_types_text = "\n".join(doc_types) if doc_types else "  (none classified yet)"

        dedup_info = result.get("deduplication", {})
        dedup_text = ""
        if dedup_info.get("enabled") and dedup_info.get("total_duplicates_removed", 0) > 0:
            dedup_text = f"\nDeduplication:\n"
            dedup_text += f"  Files uploaded: {result.get('files_uploaded', len(result['files_saved']))}\n"
            dedup_text += f"  Files saved: {len(result['files_saved'])}\n"
            dedup_text += f"  Duplicates removed: {dedup_info['total_duplicates_removed']}\n"

            if dedup_info.get("duplicate_filenames_skipped"):
                dedup_text += f"  - Duplicate filenames: {', '.join(dedup_info['duplicate_filenames_skipped'])}\n"

            if dedup_info.get("duplicate_content_detected"):
                content_dupes = [f"{k} (same as {v})" for k, v in dedup_info['duplicate_content_detected'].items()]
                dedup_text += f"  - Duplicate content: {', '.join(content_dupes)}\n"

        return f"""âœ“ Session Created from Uploads

Session ID: {result['session_id']}
Project: {project_name}
Methodology: {methodology}
Temp Directory: {result['temp_directory']}

Files Uploaded ({len(result['files_saved'])}):
{files_list}
{dedup_text}
Document Discovery Complete:
  Found: {result['documents_found']} document(s)
  Classified: {result['documents_classified']}

Classification Summary:
{doc_types_text}

Next Steps:
  - Run extract_evidence("{result['session_id']}") to map requirements to documents
  - Or use /evidence-extraction prompt for guided workflow
"""

    except ValueError as e:
        logger.warning(f"Validation error in create_session_from_uploads: {e}")
        return f"âœ— Validation Error: {str(e)}"

    except Exception as e:
        logger.error(f"Failed to create session from uploads: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


@mcp.tool()
async def upload_additional_files(
    session_id: str,
    files: list[dict],
) -> str:
    """Add additional files to an existing session.

    Accepts file content in two flexible formats:
    1. Base64 format: {"filename": "doc.pdf", "content_base64": "JVBERi0..."}
    2. Path format: {"filename": "doc.pdf", "path": "/absolute/path/to/file.pdf"}

    Adds files to an existing session's document directory and re-runs document discovery.
    Useful for iterative document submission.

    Args:
        session_id: Existing session identifier
        files: List of file objects. Each file must have:
            - filename OR name (required): Name of the file
            - content_base64 OR path (required): Either base64-encoded content or absolute file path
            - mime_type (optional): MIME type

    Returns:
        Success message with updated document discovery results

    Examples:
        # Base64 format
        upload_additional_files("session-abc123",
            [{"filename": "Baseline.pdf", "content_base64": "JVBERi0..."}])

        # Path format
        upload_additional_files("session-abc123",
            [{"name": "Baseline.pdf", "path": "/media/uploads/agents/abc/baseline.pdf"}])
    """
    try:
        logger.info(f"Uploading additional files to session: {session_id}")

        result = await upload_tools.upload_additional_files(
            session_id=session_id,
            files=files,
        )

        logger.info(f"Added {len(result['files_added'])} files to session {session_id}")

        files_list = "\n".join(f"  - {f}" for f in result["files_added"])

        doc_types = []
        for doc_type, count in sorted(result["documents_by_type"].items()):
            doc_types.append(f"  - {doc_type}: {count}")
        doc_types_text = "\n".join(doc_types) if doc_types else "  (none classified)"

        return f"""âœ“ Files Added to Session

Session ID: {session_id}

Files Added ({len(result['files_added'])}):
{files_list}

Updated Document Discovery:
  Total Found: {result['documents_found']} document(s)
  Classified: {result['documents_classified']}

Classification Summary:
{doc_types_text}

Next Steps:
  - Review the updated document list
  - Run extract_evidence("{session_id}") to re-extract evidence with new files
"""

    except mcp_errors.SessionNotFoundError:
        logger.warning(f"Session not found: {session_id}")
        return f"âœ— Session not found: {session_id}\n\nUse list_sessions to see available sessions."

    except ValueError as e:
        logger.warning(f"Validation error in upload_additional_files: {e}")
        return f"âœ— Validation Error: {str(e)}"

    except Exception as e:
        logger.error(f"Failed to upload additional files: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


@mcp.tool()
async def start_review_from_uploads(
    project_name: str,
    files: list[dict],
    methodology: str = "soil-carbon-v1.2.2",
    project_id: str | None = None,
    proponent: str | None = None,
    crediting_period: str | None = None,
    auto_extract: bool = True,
    deduplicate: bool = True,
    force_new_session: bool = False,
) -> str:
    """One-step tool to create session, upload files, and optionally extract evidence.

    This is the recommended tool for new integrations - it combines session creation,
    document discovery, and evidence extraction into a single streamlined operation.

    Accepts file content in two flexible formats:
    1. Base64 format: {"filename": "doc.pdf", "content_base64": "JVBERi0..."}
    2. Path format: {"filename": "doc.pdf", "path": "/absolute/path/to/file.pdf"}

    Args:
        project_name: Name of the project being reviewed
        files: List of file objects. Each file must have:
            - filename OR name (required): Name of the file
            - content_base64 OR path (required): Either base64-encoded content or absolute file path
            - mime_type (optional): MIME type
        methodology: Methodology identifier (default: soil-carbon-v1.2.2)
        project_id: Project ID if known (e.g., C06-4997)
        proponent: Name of project proponent
        crediting_period: Crediting period description (e.g., "2022-2032")
        auto_extract: Automatically run evidence extraction (default: True)
        deduplicate: Automatically remove duplicate files by filename and content hash (default: True)
        force_new_session: Force creation of new session even if existing session detected (default: False)

    Returns:
        Combined results from session creation and evidence extraction.
        If existing session detected, returns that session's information.

    Examples:
        # Base64 format
        start_review_from_uploads("Botany Farm 2022", [
            {"filename": "ProjectPlan.pdf", "content_base64": "JVBERi0..."},
            {"filename": "Baseline.pdf", "content_base64": "JVBERi0..."}
        ])

        # Path format (e.g., from ElizaOS)
        start_review_from_uploads("Botany Farm 2022", [
            {"name": "ProjectPlan.pdf", "path": "/media/uploads/agents/abc/plan.pdf"},
            {"name": "Baseline.pdf", "path": "/media/uploads/agents/abc/baseline.pdf"}
        ])
    """
    try:
        logger.info(f"Starting review from uploads: {project_name}")

        result = await upload_tools.start_review_from_uploads(
            project_name=project_name,
            files=files,
            methodology=methodology,
            project_id=project_id,
            proponent=proponent,
            crediting_period=crediting_period,
            auto_extract=auto_extract,
            deduplicate=deduplicate,
            force_new_session=force_new_session,
        )

        session_result = result["session_creation"]
        session_id = session_result["session_id"]

        logger.info(f"Review started from uploads: {session_id}")

        files_list = "\n".join(f"  - {f}" for f in session_result["files_saved"])

        doc_types = []
        for doc_type, count in sorted(session_result["documents_by_type"].items()):
            doc_types.append(f"  - {doc_type}: {count}")
        doc_types_text = "\n".join(doc_types) if doc_types else "  (none classified)"

        dedup_info = session_result.get("deduplication", {})
        dedup_text = ""
        if dedup_info.get("enabled") and dedup_info.get("total_duplicates_removed", 0) > 0:
            dedup_text = f"\nDeduplication:\n"
            dedup_text += f"  Files uploaded: {session_result.get('files_uploaded', len(session_result['files_saved']))}\n"
            dedup_text += f"  Files saved: {len(session_result['files_saved'])}\n"
            dedup_text += f"  Duplicates removed: {dedup_info['total_duplicates_removed']}\n"

            if dedup_info.get("duplicate_filenames_skipped"):
                dedup_text += f"  - Duplicate filenames: {', '.join(dedup_info['duplicate_filenames_skipped'])}\n"

            if dedup_info.get("duplicate_content_detected"):
                content_dupes = [f"{k} (same as {v})" for k, v in dedup_info['duplicate_content_detected'].items()]
                dedup_text += f"  - Duplicate content: {', '.join(content_dupes)}\n"

        output = f"""âœ“ Review Started from Uploads

Session ID: {session_id}
Project: {project_name}
Methodology: {methodology}

Files Uploaded ({len(session_result['files_saved'])}):
{files_list}
{dedup_text}
Documents Found: {session_result['documents_found']}
Classification Summary:
{doc_types_text}
"""

        if "evidence_extraction" in result:
            evidence = result["evidence_extraction"]

            if evidence.get("success") is False:
                output += f"""
âš ï¸  Evidence Extraction Failed:
{evidence.get('message', 'Unknown error')}

You can run extract_evidence("{session_id}") manually to retry.
"""
            else:
                output += f"""
Evidence Extraction Complete:
  Total Requirements: {evidence.get('requirements_total', 0)}
  âœ… Covered: {evidence.get('requirements_covered', 0)} ({evidence.get('requirements_covered', 0) / max(evidence.get('requirements_total', 1), 1) * 100:.1f}%)
  âš ï¸  Partial: {evidence.get('requirements_partial', 0)} ({evidence.get('requirements_partial', 0) / max(evidence.get('requirements_total', 1), 1) * 100:.1f}%)
  âŒ Missing: {evidence.get('requirements_missing', 0)} ({evidence.get('requirements_missing', 0) / max(evidence.get('requirements_total', 1), 1) * 100:.1f}%)
  Overall Coverage: {evidence.get('overall_coverage', 0) * 100:.1f}%

Next Steps:
  - Review evidence details with map_requirement("{session_id}", "REQ-###")
  - Run /cross-validation prompt to verify consistency across documents
  - Generate final report with /report-generation prompt
"""

        return output

    except ValueError as e:
        logger.warning(f"Validation error in start_review_from_uploads: {e}")
        return f"âœ— Validation Error: {str(e)}"

    except Exception as e:
        logger.error(f"Failed to start review from uploads: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


# ============================================================================
# Document Processing Tools
# ============================================================================


@mcp.tool()
async def discover_documents(session_id: str) -> str:
    """Discover and classify all documents in the project directory.

    Args:
        session_id: Unique session identifier

    Returns:
        Summary of discovered documents with classifications
    """
    try:
        logger.info(f"Discovering documents for session: {session_id}")

        results = await document_tools.discover_documents(session_id)

        summary_lines = []
        for doc_type, count in sorted(results["classification_summary"].items()):
            summary_lines.append(f"  - {doc_type}: {count}")

        summary = "\n".join(summary_lines) if summary_lines else "  (none found)"

        return f"""âœ“ Document Discovery Complete

Session: {session_id}
Documents Found: {results['documents_found']}

Classification Summary:
{summary}

Use the /document-discovery prompt for detailed results and next steps.
"""

    except mcp_errors.SessionNotFoundError:
        return f"âœ— Session not found: {session_id}"

    except Exception as e:
        logger.error(f"Failed to discover documents: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


@mcp.tool()
async def extract_pdf_text(
    filepath: str,
    start_page: int | None = None,
    end_page: int | None = None,
    extract_tables: bool = False,
) -> str:
    """Extract text content from a PDF file.

    Args:
        filepath: Absolute path to PDF file
        start_page: Optional starting page (1-indexed)
        end_page: Optional ending page (1-indexed)
        extract_tables: Whether to extract tables

    Returns:
        Extracted text content with metadata
    """
    try:
        logger.info(f"Extracting PDF text: {filepath}")

        page_range = (start_page, end_page) if start_page and end_page else None
        results = await document_tools.extract_pdf_text(
            filepath, page_range, extract_tables
        )

        pages_text = f"Pages {start_page}-{end_page}" if page_range else "All pages"
        tables_info = f", {len(results.get('tables', []))} tables found" if extract_tables else ""

        response = f"""âœ“ PDF Text Extraction Complete

File: {filepath}
Pages: {pages_text}
Total Pages: {results['page_count']}
Characters Extracted: {len(results['full_text'])}{tables_info}

"""

        preview = results['full_text'][:2000]
        if len(results['full_text']) > 2000:
            preview += f"\n\n... ({len(results['full_text']) - 2000} more characters)"

        response += f"Text Preview:\n{preview}"

        return response

    except Exception as e:
        logger.error(f"Failed to extract PDF text: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


@mcp.tool()
async def extract_gis_metadata(filepath: str) -> str:
    """Extract metadata from a GIS shapefile or GeoJSON.

    Args:
        filepath: Absolute path to GIS file (.shp or .geojson)

    Returns:
        GIS metadata including CRS, bounds, feature count
    """
    try:
        logger.info(f"Extracting GIS metadata: {filepath}")

        results = await document_tools.extract_gis_metadata(filepath)

        return f"""âœ“ GIS Metadata Extraction Complete

File: {filepath}
Driver: {results.get('driver', 'Unknown')}
CRS: {results.get('crs', 'Unknown')}
Geometry Type: {results.get('geometry_type', 'Unknown')}
Feature Count: {results.get('feature_count', 0)}
Bounds: {results.get('bounds', 'Unknown')}

Schema: {results.get('schema', 'Not available')}
"""

    except Exception as e:
        logger.error(f"Failed to extract GIS metadata: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


# ============================================================================
# Evidence Extraction Tools (Phase 3)
# ============================================================================


@mcp.tool()
async def extract_evidence(session_id: str) -> str:
    """Extract evidence for all requirements from discovered documents.

    This tool maps each requirement in the checklist to relevant documents,
    extracts evidence snippets with page citations, and calculates coverage statistics.

    Args:
        session_id: Unique session identifier

    Returns:
        Summary of evidence extraction results with coverage statistics
    """
    try:
        logger.info(f"Extracting evidence for session: {session_id}")

        results = await evidence_tools.extract_all_evidence(session_id)

        return f"""âœ“ Evidence Extraction Complete

Session: {session_id}

Coverage Summary:
  Total Requirements: {results['requirements_total']}
  âœ… Covered: {results['requirements_covered']} ({results['requirements_covered']/results['requirements_total']*100:.1f}%)
  âš ï¸  Partial: {results['requirements_partial']} ({results['requirements_partial']/results['requirements_total']*100:.1f}%)
  âŒ Missing: {results['requirements_missing']} ({results['requirements_missing']/results['requirements_total']*100:.1f}%)
  Overall Coverage: {results['overall_coverage']*100:.1f}%

Results saved to: evidence.json
Session updated with coverage statistics

Use the /evidence-extraction prompt for detailed results and next steps.
"""

    except mcp_errors.SessionNotFoundError:
        return f"âœ— Session not found: {session_id}"

    except Exception as e:
        logger.error(f"Failed to extract evidence: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


@mcp.tool()
async def map_requirement(session_id: str, requirement_id: str) -> str:
    """Map a single requirement to documents and extract evidence.

    Args:
        session_id: Unique session identifier
        requirement_id: Requirement ID (e.g., "REQ-002", "REQ-007")

    Returns:
        Evidence details for the specific requirement
    """
    try:
        logger.info(f"Mapping requirement {requirement_id} for session: {session_id}")

        result = await evidence_tools.map_requirement(session_id, requirement_id)

        status_emoji = {
            "covered": "âœ…",
            "partial": "âš ï¸",
            "missing": "âŒ",
            "flagged": "ðŸš©"
        }

        emoji = status_emoji.get(result['status'], "â“")

        response = f"""âœ“ Requirement Mapping Complete

{emoji} {result['requirement_id']}: {result['requirement_text']}

Status: {result['status'].upper()}
Confidence: {result['confidence']:.2f}
Category: {result['category']}

Mapped Documents ({len(result['mapped_documents'])}):
"""

        for doc in result['mapped_documents'][:5]:
            response += f"  - {doc['document_name']} (relevance: {doc['relevance_score']:.2f})\n"

        if len(result['mapped_documents']) > 5:
            response += f"  ... and {len(result['mapped_documents']) - 5} more\n"

        response += f"\nEvidence Snippets ({len(result['evidence_snippets'])}):\n"

        for idx, snippet in enumerate(result['evidence_snippets'][:3], 1):
            page_info = f" (page {snippet['page']})" if snippet.get('page') else ""
            section_info = f" - {snippet['section']}" if snippet.get('section') else ""
            response += f"\n{idx}. {snippet['document_name']}{page_info}{section_info}\n"
            response += f"   {snippet['text'][:150]}...\n"
            response += f"   Confidence: {snippet['confidence']:.2f}\n"

        if len(result['evidence_snippets']) > 3:
            response += f"\n... and {len(result['evidence_snippets']) - 3} more snippets\n"

        return response

    except mcp_errors.SessionNotFoundError:
        return f"âœ— Session not found: {session_id}"

    except ValueError as e:
        return f"âœ— {str(e)}"

    except Exception as e:
        logger.error(f"Failed to map requirement: {e}", exc_info=True)
        return f"âœ— Error: {str(e)}"


# ============================================================================
# Prompts
# ============================================================================


@mcp.prompt()
def list_capabilities() -> list[TextContent]:
    """List all MCP server capabilities, tools, and workflow prompts"""
    capabilities = f"""# Regen Registry Review MCP Server

**Version:** 2.0.0
**Purpose:** Automate carbon credit project registry review workflows

## Capabilities

### Tools Available

**Session Management:**
- `start_review` - Quick-start: Create session and discover documents in one step
- `create_session` - Create new review session
- `load_session` - Load existing session
- `update_session_state` - Update session progress
- `list_sessions` - List all sessions
- `delete_session` - Delete a session

**Document Processing (Phase 2):**
- `discover_documents` - Scan and classify project documents
- `extract_pdf_text` - Extract text from PDF files
- `extract_gis_metadata` - Extract GIS shapefile metadata

**Evidence Extraction (Phase 3):**
- `extract_evidence` - Map all requirements to documents and extract evidence
- `map_requirement` - Map a single requirement to documents with evidence snippets

**Validation & Reporting (Phase 4):**
- `cross_validation` - Validate dates, land tenure, and project IDs across documents
- `generate_report` - Create comprehensive Markdown and JSON reports

### Workflows (7 Sequential Stages)

1. **Initialize** (`/initialize`) - Create session and load checklist
2. **Document Discovery** (`/document-discovery`) - Scan and classify all documents
3. **Evidence Extraction** (`/evidence-extraction`) - Map requirements to evidence
4. **Cross-Validation** (`/cross-validation`) - Verify consistency across documents
5. **Report Generation** (`/report-generation`) - Generate structured review report
6. **Human Review** (`/human-review`) - Present flagged items for decision
7. **Complete** (`/complete`) - Finalize and export report

### Supported Methodology

- Soil Carbon v1.2.2 (GHG Benefits in Managed Crop and Grassland Systems)
- Additional methodologies: Coming in Phase 3+

### File Types Supported

- PDF documents (text and tables)
- GIS shapefiles (.shp, .shx, .dbf, .geojson)
- Imagery files (.tif, .tiff) - metadata only

## Getting Started

**Quick Start (Recommended):**
```
start_review(
    project_name="Botany Farm",
    documents_path="/absolute/path/to/documents",
    methodology="soil-carbon-v1.2.2"
)
```
This creates a session and discovers all documents in one step.

**Or use the prompts directly:**
```
/document-discovery
/evidence-extraction
```
These will auto-select your most recent session, or guide you to create one if none exists.

**Manual workflow:**
1. Create a session with `create_session`
2. Discover documents with `discover_documents` or `/document-discovery` prompt
3. Extract evidence with `extract_evidence` or `/evidence-extraction` prompt

## Status

- **Phase 1 (Foundation):** âœ… Complete (Session management, infrastructure)
- **Phase 2 (Document Processing):** âœ… Complete (Document discovery, classification, PDF/GIS extraction)
- **Phase 3 (Evidence Extraction):** âœ… Complete (Requirement mapping, evidence snippets, coverage analysis)
- **Phase 4 (Validation & Reporting):** âœ… Complete (Cross-validation, report generation, LLM-native extraction)
- **Phase 5 (Integration & Polish):** ðŸš§ In Progress (All 7 prompts complete, testing in progress)

**Test Coverage:** 120/120 tests passing (100%)
**LLM Extraction:** 80%+ recall with caching and cost tracking
**Last Updated:** {datetime.now().strftime("%Y-%m-%d %H:%M UTC")}
"""

    return [TextContent(type="text", text=capabilities)]


@mcp.prompt(name="list-sessions")
async def list_sessions_prompt() -> list[TextContent]:
    """List all active review sessions with project names, progress, and workflow status"""
    result = await list_sessions()
    return [TextContent(type="text", text=result)]


@mcp.prompt(name="list-projects")
async def list_projects_prompt() -> list[TextContent]:
    """List available example projects in the examples directory for quick testing"""
    result = await list_example_projects()
    return [TextContent(type="text", text=result)]


@mcp.prompt(name="A-initialize")
async def initialize(project: str) -> list[TextContent]:
    """Initialize new review session - provide 'project name, /path/to/documents' (comma-separated)"""
    parts = [p.strip() for p in project.split(",", 1)]
    project_name = parts[0] if len(parts) > 0 else None
    documents_path = parts[1] if len(parts) > 1 else None
    return await A_initialize.initialize_prompt(project_name, documents_path)


@mcp.prompt(name="B-document-discovery")
async def document_discovery(project: str = "") -> list[TextContent]:
    """Discover and classify project documents - provide session ID or project name (optional, auto-selects latest)"""
    session_id = project if project and not "," in project else None
    project_name = None
    documents_path = None
    if "," in project:
        parts = [p.strip() for p in project.split(",", 1)]
        project_name = parts[0]
        documents_path = parts[1] if len(parts) > 1 else None
    return await B_document_discovery.document_discovery_prompt(project_name, documents_path, session_id)


@mcp.prompt(name="C-evidence-extraction")
async def evidence_extraction(project: str = "") -> list[TextContent]:
    """Extract evidence from documents and map to requirements - provide session ID (optional, auto-selects latest)"""
    session_id = project if project else None
    result = await C_evidence_extraction.evidence_extraction(session_id)
    return [TextContent(type="text", text=result)]


@mcp.prompt(name="D-cross-validation")
async def cross_validation(project: str = "") -> list[TextContent]:
    """Validate consistency across documents (dates, land tenure, project IDs) - provide session ID (optional)"""
    session_id = project if project else None
    return await D_cross_validation.cross_validation_prompt(session_id)


@mcp.prompt(name="E-report-generation")
async def report_generation(project: str = "") -> list[TextContent]:
    """Generate complete review report in Markdown and JSON formats - provide session ID (optional)"""
    session_id = project if project else None
    return await E_report_generation.report_generation_prompt(session_id)


@mcp.prompt(name="F-human-review")
async def human_review(project: str = "") -> list[TextContent]:
    """Review flagged validation items requiring human judgment - provide session ID (optional)"""
    session_id = project if project else None
    return await F_human_review.human_review_prompt(session_id)


@mcp.prompt(name="G-complete")
async def complete(project: str = "") -> list[TextContent]:
    """Complete and finalize the review session with final assessment - provide session ID (optional)"""
    session_id = project if project else None
    return await G_complete.complete_prompt(session_id)


# ============================================================================
# Entry Point
# ============================================================================


def main():
    """Main entry point for the MCP server."""
    logger.info("Starting Registry Review MCP Server")
    logger.info(f"Data directory: {settings.data_dir}")
    logger.info(f"Log level: {settings.log_level}")

    try:
        mcp.run()
    except KeyboardInterrupt:
        logger.info("Server stopped by user")
    except Exception as e:
        logger.error(f"Server error: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()
