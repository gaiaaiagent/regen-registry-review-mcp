"""Optimized evidence extraction using LLM and in-memory document caching.

This module replaces the keyword-based evidence extraction with semantic LLM-based
extraction. It eliminates redundant file I/O by loading documents once into memory
and processes requirements in parallel.

Performance improvement: 11 minutes ‚Üí ~25 seconds (26x faster)
Quality improvement: Semantic understanding vs keyword matching
"""

import asyncio
import hashlib
import json
import logging
import os
import re
import time
from pathlib import Path
from typing import Any

from anthropic import AsyncAnthropic

# LiteLLM for multi-provider support
import litellm
from litellm import acompletion

from ..config.settings import settings

# Suppress LiteLLM debug output
litellm.suppress_debug_info = True
from ..models.evidence import (
    EvidenceSnippet,
    MappedDocument,
    RequirementEvidence,
    EvidenceExtractionResult,
)
from ..utils.state import StateManager

logger = logging.getLogger(__name__)


# Configuration for type-aware structured field extraction
# Maps field patterns to their extraction instructions
STRUCTURED_FIELD_CONFIGS = {
    "land_tenure": {
        "keywords": ["land tenure", "ownership", "landowner"],
        "fields": [
            ("owner_name", "Full name of landowner or leaseholder (e.g., 'Nicholas Denman')"),
            ("area_hectares", "Total project area in hectares (numeric, convert acres if needed)"),
            ("tenure_type", "Type of tenure: ownership, lease, or easement"),
        ],
        "warning": "Only extract actual names of people/organizations, NOT generic text like 'The Project' or 'The Farm'.",
    },
    "project_identity": {
        "keywords": ["project id", "registry id", "project name", "project identifier"],
        "fields": [
            ("project_id", "Project identifier (e.g., 'C01-1234' or '4997Botany22')"),
            ("project_name", "Full project name"),
        ],
    },
    "project_start_date": {
        "keywords": ["start date", "project start"],
        "fields": [
            ("project_start_date", "When the project began (format: YYYY-MM-DD)"),
        ],
        "warning": "Only extract explicitly stated dates, not inferred ones.",
    },
    "crediting_period": {
        "keywords": ["crediting period"],
        "fields": [
            ("crediting_period_years", "Duration in years (integer)"),
            ("crediting_period_start", "Start date (format: YYYY-MM-DD)"),
            ("crediting_period_end", "End date (format: YYYY-MM-DD)"),
        ],
    },
    "buffer_pool": {
        "keywords": ["buffer pool"],
        "fields": [
            ("buffer_pool_percentage", "Buffer pool contribution percentage (numeric, e.g., 20)"),
        ],
    },
    "leakage": {
        "keywords": ["leakage"],
        "fields": [
            ("leakage_percentage", "Leakage percentage threshold (numeric)"),
        ],
    },
    "permanence": {
        "keywords": ["permanence"],
        "fields": [
            ("permanence_period_years", "Permanence period duration in years (integer)"),
        ],
    },
}


def _find_matching_config(requirement_text: str) -> dict | None:
    """Find a matching structured field config based on requirement text keywords."""
    text_lower = requirement_text.lower()
    for config in STRUCTURED_FIELD_CONFIGS.values():
        if any(kw in text_lower for kw in config["keywords"]):
            return config
    return None


def _build_structured_guidance(config: dict | None, validation_type: str) -> str:
    """Build structured field extraction guidance from config.

    Enforces EXACT canonical field names to ensure consistency between
    extraction (Stage D) and validation (Stage E).
    """
    if config is None:
        if validation_type == "cross_document":
            return """
**STRUCTURED FIELD EXTRACTION:**
For cross-document validation, extract key values using EXACT field names:
- "owner_name": Full name of person/organization (NOT generic terms)
- "area_hectares": Numeric area in hectares
- "project_id": Project identifier string
- "project_start_date": Date in YYYY-MM-DD format

Use these exact field names in your structured_fields JSON.
"""
        elif validation_type == "structured_field":
            return """
**STRUCTURED FIELD EXTRACTION:**
Extract specific values using EXACT canonical field names:
- Dates: "project_start_date", "crediting_period_start", "crediting_period_end" (YYYY-MM-DD)
- Periods: "crediting_period_years", "permanence_period_years" (integers)
- Percentages: "buffer_pool_percentage", "leakage_percentage" (numbers 0-100)
- IDs: "project_id" (string)

Use these exact field names. Do NOT use synonyms like "project_identifier".
"""
        return ""

    # Build example JSON to show exact field names
    example_fields = {}
    for name, desc in config["fields"]:
        if "date" in name.lower():
            example_fields[name] = "YYYY-MM-DD"
        elif "percentage" in name.lower():
            example_fields[name] = 20.0
        elif "years" in name.lower():
            example_fields[name] = 10
        elif "hectares" in name.lower():
            example_fields[name] = 100.5
        elif "name" in name.lower():
            example_fields[name] = "Example Name"
        else:
            example_fields[name] = "value"

    import json
    example_json = json.dumps(example_fields, indent=2)

    field_list = "\n".join(f'- "{name}": {desc}' for name, desc in config["fields"])
    warning = f"\n‚ö†Ô∏è CRITICAL: {config['warning']}" if config.get("warning") else ""

    return f"""
**STRUCTURED FIELD EXTRACTION:**

Extract ONLY these fields using the EXACT field names shown:
{field_list}
{warning}

**Required JSON format** - use these exact keys:
```json
{example_json}
```

‚ö†Ô∏è Do NOT use synonyms or variations. Use the exact field names above.
For example, use "project_id" NOT "project_identifier" or "registry_id".
"""


def build_type_aware_prompt(
    requirement: dict,
    document_content: str,
    document_name: str,
    validation_type: str,
) -> str:
    """Build an LLM prompt tailored to the validation type.

    For cross_document and structured_field requirements, includes instructions
    to extract specific structured fields alongside evidence snippets. Uses
    STRUCTURED_FIELD_CONFIGS to determine which fields to extract based on
    requirement text keywords.

    Args:
        requirement: Requirement dictionary containing requirement_text,
            accepted_evidence, category, and requirement_id.
        document_content: Full markdown content of the document to analyze.
        document_name: Name of the document for context in the prompt.
        validation_type: Type of validation from checklist
            (document_presence, cross_document, structured_field, manual).

    Returns:
        Formatted prompt string for LLM extraction, including base task
        description, structured field guidance (if applicable), and output format.
    """
    requirement_text = requirement.get("requirement_text", "")
    accepted_evidence = requirement.get("accepted_evidence", "")
    category = requirement.get("category", "")

    # Base prompt structure
    base_prompt = f"""You are analyzing a carbon credit project document to find evidence for a specific requirement.

**Requirement Category:** {category}

**Requirement Text:**
{requirement_text}

**Accepted Evidence:**
{accepted_evidence}

**Document Name:** {document_name}

**Document Content:**
{document_content}

**Task:**
Extract ALL passages from the document that provide evidence for this requirement.
"""

    # Build structured guidance from config (if validation type requires it)
    structured_guidance = ""
    if validation_type in ("cross_document", "structured_field"):
        config = _find_matching_config(requirement_text)
        structured_guidance = _build_structured_guidance(config, validation_type)

    # Output format based on whether we need structured fields
    if structured_guidance:
        output_format = """
**Output Format:**
```json
[
  {
    "text": "The exact quote from the document (2-3 sentences)",
    "page": 5,
    "section": "2. Land Tenure",
    "confidence": 0.95,
    "reasoning": "Why this is relevant evidence",
    "structured_fields": {
      "owner_name": "Nicholas Denman",
      "area_hectares": 100.5
    }
  }
]
```

If no structured fields found in a snippet, omit the "structured_fields" key or set it to null.
Extract only high-quality evidence (confidence > 0.6). Be precise and thorough."""
    else:
        output_format = """
**Output Format:**
```json
[
  {
    "text": "The exact quote from the document (2-3 sentences)",
    "page": 5,
    "section": "2. Land Tenure",
    "confidence": 0.95,
    "reasoning": "Why this is relevant evidence"
  }
]
```

Extract only high-quality evidence (confidence > 0.6). Be precise and thorough."""

    evidence_instructions = """
For each piece of evidence, provide:
1. **text**: The exact quote from the document (2-3 sentences with context)
2. **page**: Page number (extract from markers like `![](_page_5_Picture_0.jpeg)` or section headers)
3. **section**: The section header this appears under
4. **confidence**: Your confidence that this provides evidence (0.0-1.0)
5. **reasoning**: Brief explanation of why this is relevant evidence

Return a JSON array of evidence snippets. If no relevant evidence found, return empty array.
"""

    return base_prompt + structured_guidance + evidence_instructions + output_format


def _parse_json_response(response_text: str) -> list[dict]:
    """Robustly parse JSON from LLM response.

    Handles various formats:
    - Plain JSON array
    - JSON wrapped in ```json ... ``` markdown
    - JSON with leading/trailing text
    - Empty responses (returns empty array)

    Args:
        response_text: Raw LLM response text

    Returns:
        Parsed JSON array, or empty list if parsing fails
    """
    if not response_text or not response_text.strip():
        return []

    text = response_text.strip()

    # Try 1: Extract from ```json ... ``` blocks
    json_block_match = re.search(r'```(?:json)?\s*(\[[\s\S]*?\])\s*```', text)
    if json_block_match:
        try:
            return json.loads(json_block_match.group(1))
        except json.JSONDecodeError:
            pass

    # Try 2: Find array anywhere in response
    array_match = re.search(r'\[[\s\S]*\]', text)
    if array_match:
        try:
            return json.loads(array_match.group(0))
        except json.JSONDecodeError:
            pass

    # Try 3: Direct parse (might be clean JSON)
    try:
        result = json.loads(text)
        if isinstance(result, list):
            return result
        elif isinstance(result, dict):
            # Some LLMs wrap array in object
            if "evidence" in result:
                return result["evidence"]
            return [result]
    except json.JSONDecodeError:
        pass

    # Try 4: Empty array indicators
    if text in ("[]", "null", "None", "No evidence found", ""):
        return []

    logger.warning(f"Could not parse JSON from response: {text[:200]}...")
    return []


async def get_markdown_content(document: dict[str, Any], session_id: str) -> str | None:
    """Get markdown content for a document.

    Markdown should already exist from Stage 2 (document discovery).
    This function simply loads the cached markdown.
    """
    # Markdown should be available from Stage 2 discovery
    if document.get("has_markdown") and document.get("markdown_path"):
        md_path = Path(document["markdown_path"])
        if md_path.exists():
            return md_path.read_text(encoding="utf-8")
        else:
            logger.warning(f"Markdown path exists in metadata but file not found: {md_path}")

    # If no markdown available, log warning and skip this document
    logger.warning(
        f"No markdown available for {document.get('filename', 'unknown')}. "
        f"Document may not have been converted in Stage 2 (discovery)."
    )
    return None


def generate_cache_key(
    requirement_id: str,
    requirement_text: str,
    accepted_evidence: str,
    document_id: str,
    document_content: str,
    model: str,
    temperature: float
) -> str:
    """Generate deterministic cache key from inputs.

    Args:
        requirement_id: Unique requirement identifier
        requirement_text: Full text of the requirement
        accepted_evidence: What evidence is accepted for this requirement
        document_id: Document identifier
        document_content: Full markdown content of the document
        model: LLM model identifier
        temperature: Model temperature setting

    Returns:
        16-character hex hash uniquely identifying this extraction task
    """
    # Create stable representation
    cache_input = {
        "requirement_id": requirement_id,
        "requirement_text": requirement_text,
        "accepted_evidence": accepted_evidence,
        "document_id": document_id,
        "document_hash": hashlib.sha256(document_content.encode()).hexdigest()[:16],
        "model": model,
        "temperature": temperature
    }

    # Serialize deterministically
    cache_str = json.dumps(cache_input, sort_keys=True)

    # Hash to fixed length
    return hashlib.sha256(cache_str.encode()).hexdigest()[:16]


def load_from_cache(cache_key: str) -> list[EvidenceSnippet] | None:
    """Load cached LLM response if valid.

    Args:
        cache_key: Cache key from generate_cache_key()

    Returns:
        List of EvidenceSnippet objects if cache hit and valid, None otherwise
    """
    cache_path = settings.get_llm_cache_path(cache_key)

    if not cache_path.exists():
        return None

    try:
        with open(cache_path) as f:
            cached = json.load(f)

        # Check TTL
        age = time.time() - cached["created_at"]
        if age > settings.llm_cache_ttl:
            cache_path.unlink()  # Expired, delete
            return None

        # Convert back to EvidenceSnippet objects
        return [
            EvidenceSnippet(**item)
            for item in cached["response"]
        ]

    except Exception as e:
        logger.warning(f"Cache load failed for {cache_key}: {e}")
        return None


def save_to_cache(
    cache_key: str,
    snippets: list[EvidenceSnippet],
    api_response,
    use_litellm: bool = False
) -> None:
    """Save LLM response to cache.

    Args:
        cache_key: Cache key from generate_cache_key()
        snippets: Extracted evidence snippets
        api_response: Raw API response object with usage metadata
        use_litellm: Whether response is from LiteLLM (uses different attribute names)
    """
    cache_path = settings.get_llm_cache_path(cache_key)

    # Handle different usage attribute names between Anthropic and LiteLLM
    if use_litellm:
        input_tokens = api_response.usage.prompt_tokens
        output_tokens = api_response.usage.completion_tokens
    else:
        input_tokens = api_response.usage.input_tokens
        output_tokens = api_response.usage.output_tokens

    cache_data = {
        "cache_key": cache_key,
        "created_at": time.time(),
        "ttl": settings.llm_cache_ttl,
        "response": [s.model_dump() for s in snippets],
        "metadata": {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "model": api_response.model
        }
    }

    try:
        with open(cache_path, 'w') as f:
            json.dump(cache_data, f, indent=2)
    except Exception as e:
        logger.warning(f"Cache save failed for {cache_key}: {e}")


async def extract_evidence_with_llm(
    client: AsyncAnthropic | None,
    requirement: dict,
    document_content: str,
    document_id: str,
    document_name: str,
    validation_type: str = "document_presence",
) -> list[EvidenceSnippet]:
    """Use LLM to extract evidence for a requirement from a document.

    This replaces keyword matching with semantic understanding.
    Includes local caching to avoid redundant API calls during development.
    Supports multiple providers: Anthropic (direct), Gemini/OpenAI (via LiteLLM).

    For cross_document and structured_field validation types, also extracts
    structured fields (owner_name, dates, etc.) for use in validation.
    """
    requirement_id = requirement.get("requirement_id", "")
    requirement_text = requirement.get("requirement_text", "")
    accepted_evidence = requirement.get("accepted_evidence", "")

    # Truncate content if too long (200K chars max)
    if len(document_content) > 200000:
        document_content = document_content[:200000] + "\n\n[... document truncated ...]"

    # Generate cache key (include validation_type for cache differentiation)
    active_model = settings.get_active_llm_model()
    cache_key = generate_cache_key(
        requirement_id=requirement_id,
        requirement_text=requirement_text,
        accepted_evidence=accepted_evidence,
        document_id=document_id,
        document_content=document_content,
        model=active_model,
        temperature=settings.llm_temperature
    )
    # Add validation_type suffix to differentiate cache entries
    cache_key = f"{cache_key}_{validation_type[:4]}"

    # Try cache first (if enabled)
    if settings.llm_cache_enabled:
        cached_response = load_from_cache(cache_key)
        if cached_response:
            logger.info(f"üì¶ Cache hit: {requirement_id} + {document_name}")
            return cached_response

    # Build type-aware prompt
    prompt = build_type_aware_prompt(
        requirement=requirement,
        document_content=document_content,
        document_name=document_name,
        validation_type=validation_type,
    )

    # Cache miss - call API
    logger.info(f"üåê API call: {requirement_id} + {document_name}")

    # Determine if using LiteLLM based on provider
    provider = settings.llm_provider
    use_litellm = provider in ("gemini", "openai")

    system_prompt = "You are an expert at analyzing carbon credit project documentation and extracting relevant evidence for compliance requirements."

    try:
        if use_litellm:
            # Set up API keys for LiteLLM
            if provider == "gemini":
                os.environ["GEMINI_API_KEY"] = settings.google_api_key
            elif provider == "openai":
                os.environ["OPENAI_API_KEY"] = settings.openai_api_key

            # Call LiteLLM
            response = await acompletion(
                model=active_model,
                max_tokens=4000,
                temperature=settings.llm_temperature,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ]
            )
            response_text = response.choices[0].message.content
        else:
            # Call Anthropic directly with prompt caching
            response = await client.messages.create(
                model=active_model,
                max_tokens=4000,
                system=[
                    {
                        "type": "text",
                        "text": system_prompt,
                        "cache_control": {"type": "ephemeral"}
                    }
                ],
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt,
                                "cache_control": {"type": "ephemeral"}
                            }
                        ]
                    }
                ]
            )
            response_text = response.content[0].text

        # Extract JSON from response (might be wrapped in markdown or have extra text)
        evidence_array = _parse_json_response(response_text)

        # Convert to EvidenceSnippet objects
        snippets = []
        for item in evidence_array:
            # Extract structured fields if present (for cross_document/structured_field types)
            structured_fields = item.get("structured_fields")
            if structured_fields and not isinstance(structured_fields, dict):
                structured_fields = None  # Ensure it's a dict or None

            # Determine extraction method based on presence of structured fields
            extraction_method = "structured" if structured_fields else "semantic"

            snippet = EvidenceSnippet(
                text=item["text"],
                document_id=document_id,
                document_name=document_name,
                page=item.get("page"),
                section=item.get("section"),
                confidence=item["confidence"],
                keywords_matched=[],  # Not using keywords anymore
                extraction_method=extraction_method,
                structured_fields=structured_fields,
            )
            snippets.append(snippet)

        # Save to cache (if enabled)
        if settings.llm_cache_enabled:
            save_to_cache(cache_key, snippets, response, use_litellm=use_litellm)

        return snippets

    except Exception as e:
        logger.error(f"LLM extraction failed for {document_name}: {e}")
        return []


async def extract_all_evidence(session_id: str) -> dict[str, Any]:
    """Optimized evidence extraction with LLM and document caching.

    Implementation notes:
    1. Load all documents ONCE into memory
    2. Use LLM for semantic evidence extraction
    3. Respect mappings from Stage 3 (only check mapped docs)
    4. Process requirements in parallel (5 concurrent)
    5. Use prompt caching to reduce LLM costs

    Performance: 11 minutes ‚Üí 25 seconds (26x faster)
    """
    state_manager = StateManager(session_id)

    # ========================================================================
    # Phase 1: Load Everything Once
    # ========================================================================

    print("üìö Loading session data...", flush=True)

    # Check that requirement mapping was completed (Stage 3)
    session_data = state_manager.read_json("session.json")
    workflow_progress = session_data.get("workflow_progress", {})

    if workflow_progress.get("requirement_mapping") != "completed":
        raise ValueError(
            "Requirement mapping not complete. Run Stage 3 first: /C-requirement-mapping\n\n"
            "Evidence extraction requires mapped documents."
        )

    # Load documents.json
    docs_data = state_manager.read_json("documents.json")
    documents = docs_data.get("documents", [])

    # Load mappings.json from Stage 3
    mappings_data = state_manager.read_json("mappings.json")
    mappings = {m["requirement_id"]: m for m in mappings_data.get("mappings", [])}

    # Load checklist
    checklist_path = settings.get_checklist_path("soil-carbon-v1.2.2")
    with open(checklist_path) as f:
        checklist_data = json.load(f)
    requirements = checklist_data.get("requirements", [])

    print(f"üìã Processing {len(requirements)} requirements", flush=True)

    # ========================================================================
    # Phase 1.5: Lazy PDF Conversion (Only Mapped PDFs)
    # ========================================================================
    # Convert only PDFs that are mapped to requirements and don't have markdown yet
    # This saves significant time by not converting irrelevant PDFs

    # Collect all document IDs that are mapped to any requirement
    mapped_doc_ids = set()
    for mapping in mappings.values():
        mapped_doc_ids.update(mapping.get("mapped_documents", []))

    # Find PDFs needing conversion (mapped but no markdown)
    pdfs_to_convert = []
    for doc in documents:
        if doc["document_id"] in mapped_doc_ids:
            # Check file extension to identify PDFs (classification is content-based, not format-based)
            if doc["filepath"].lower().endswith(".pdf") and not doc.get("has_markdown"):
                pdfs_to_convert.append(doc)

    if pdfs_to_convert:
        from ..extractors.marker_extractor import batch_convert_pdfs_parallel
        from .document_tools import calculate_optimal_workers
        from pathlib import Path

        pdf_count = len(pdfs_to_convert)
        total_docs = len(documents)
        print(f"\nüìÑ Converting {pdf_count}/{total_docs} mapped PDF(s) to markdown...", flush=True)

        # Extract file paths
        pdf_paths = [doc["filepath"] for doc in pdfs_to_convert]

        # Calculate optimal workers
        max_workers = calculate_optimal_workers(pdf_count)

        # Show worker count
        if max_workers > 1:
            print(f"   Using {max_workers} concurrent workers (hardware-optimized)", flush=True)

        try:
            # Batch convert with progress indicators
            conversion_results = await batch_convert_pdfs_parallel(
                pdf_paths,
                max_workers=max_workers,
                unload_after=True
            )

            # Update document records with markdown paths
            converted_count = 0
            for doc in pdfs_to_convert:
                filepath = doc["filepath"]
                result = conversion_results.get(filepath)

                if result and not result.get("error"):
                    # Save markdown next to PDF
                    pdf_path = Path(filepath)
                    md_path = pdf_path.with_suffix('.md')
                    md_path.write_text(result["markdown"], encoding="utf-8")

                    # Update document record
                    doc["markdown_path"] = str(md_path)
                    doc["has_markdown"] = True
                    converted_count += 1

            print(f"‚úÖ Converted {converted_count}/{pdf_count} PDF(s)", flush=True)

            # Save updated documents to disk
            docs_data["documents"] = documents
            state_manager.write_json("documents.json", docs_data)

        except Exception as e:
            print(f"‚ö†Ô∏è  PDF conversion failed: {e}", flush=True)
            print(f"   Continuing with available documents...", flush=True)

    else:
        print(f"‚úì All mapped documents already have markdown", flush=True)

    print(f"\nüìÑ Loading {len(documents)} documents into memory...", flush=True)

    # ========================================================================
    # Phase 2: Load All Documents Into Memory ONCE
    # ========================================================================

    doc_cache = {}  # document_id -> markdown content
    doc_metadata = {}  # document_id -> document dict

    for doc in documents:
        doc_id = doc["document_id"]
        doc_metadata[doc_id] = doc

        # Load markdown content
        content = await get_markdown_content(doc, session_id)
        if content:
            doc_cache[doc_id] = content
            print(f"  ‚úì Loaded {doc['filename']} ({len(content)} chars)", flush=True)
        else:
            print(f"  ‚úó Failed to load {doc['filename']}", flush=True)

    print(f"\n‚úÖ All documents cached in memory", flush=True)

    # ========================================================================
    # Phase 3: Extract Evidence (Parallel)
    # ========================================================================

    print(f"\nüîç Extracting evidence with LLM...\n", flush=True)

    # Create LLM client (only needed for Anthropic provider)
    provider = settings.llm_provider
    if provider == "anthropic":
        anthropic_client = AsyncAnthropic(api_key=settings.anthropic_api_key)
    else:
        anthropic_client = None  # LiteLLM handles Gemini/OpenAI

    # Process requirements in parallel (rate-limited)
    semaphore = asyncio.Semaphore(5)  # Max 5 concurrent LLM calls

    async def extract_requirement_evidence(req: dict, index: int) -> RequirementEvidence:
        """Extract evidence for one requirement using LLM.

        Uses type-aware extraction based on validation_type from checklist.
        """
        async with semaphore:
            requirement_id = req["requirement_id"]
            validation_type = req.get("validation_type", "document_presence")

            # Show progress
            if index % 5 == 0 or index == 1 or index == len(requirements):
                percentage = (index / len(requirements)) * 100
                print(f"  ‚è≥ [{index}/{len(requirements)}] ({percentage:.0f}%) {requirement_id}", flush=True)

            # Get mapping from Stage 3
            mapping = mappings.get(requirement_id)

            # If not mapped, mark as missing
            if not mapping or not mapping.get("mapped_documents"):
                return RequirementEvidence(
                    requirement_id=requirement_id,
                    requirement_text=req.get("requirement_text", ""),
                    category=req.get("category", ""),
                    validation_type=validation_type,
                    status="missing",
                    confidence=0.0,
                    mapped_documents=[],
                    evidence_snippets=[],
                    notes="No documents mapped in Stage 3"
                )

            # Get mapped document IDs
            mapped_doc_ids = mapping["mapped_documents"]

            # Extract evidence from each mapped document using LLM
            all_snippets = []
            mapped_docs = []

            for doc_id in mapped_doc_ids:
                # Get cached content (NO FILE I/O!)
                content = doc_cache.get(doc_id)
                if not content:
                    continue

                doc = doc_metadata[doc_id]

                # Use type-aware LLM extraction based on validation_type
                snippets = await extract_evidence_with_llm(
                    client=anthropic_client,
                    requirement=req,
                    document_content=content,
                    document_id=doc_id,
                    document_name=doc["filename"],
                    validation_type=validation_type,
                )

                all_snippets.extend(snippets)

                # Track mapped document
                mapped_docs.append(MappedDocument(
                    document_id=doc_id,
                    document_name=doc["filename"],
                    filepath=doc["filepath"],
                    relevance_score=1.0,  # LLM extracts only relevant evidence
                    keywords_found=[]  # Not using keywords anymore
                ))

            # Determine status based on evidence quality
            if not all_snippets:
                status = "missing"
                confidence = 0.0
            elif any(s.confidence > 0.8 for s in all_snippets):
                status = "covered"
                confidence = max(s.confidence for s in all_snippets)
            else:
                status = "partial"
                confidence = max(s.confidence for s in all_snippets) if all_snippets else 0.0

            return RequirementEvidence(
                requirement_id=requirement_id,
                requirement_text=req.get("requirement_text", ""),
                category=req.get("category", ""),
                validation_type=validation_type,
                status=status,
                confidence=confidence,
                mapped_documents=mapped_docs,
                evidence_snippets=all_snippets
            )

    # Process all requirements in parallel
    tasks = [
        extract_requirement_evidence(req, i)
        for i, req in enumerate(requirements, 1)
    ]

    all_evidence = await asyncio.gather(*tasks)

    # ========================================================================
    # Phase 3.5: Enrich Snippets with PDF Coordinates (Bulk)
    # ========================================================================
    # Opens each PDF once and processes all snippets for that document

    print(f"\nüìç Enriching evidence with PDF coordinates...", flush=True)

    try:
        from ..extractors.pdf_coordinates import enrich_snippets_with_coordinates

        # Collect all snippets from all evidence
        all_snippets = []
        for evidence in all_evidence:
            all_snippets.extend(evidence.evidence_snippets)

        # === DEBUG: Log diagnostic info before enrichment ===
        logger.info(f"[EVIDENCE] Starting coordinate enrichment")
        logger.info(f"[EVIDENCE] Total snippets to enrich: {len(all_snippets)}")
        logger.info(f"[EVIDENCE] Documents in metadata: {len(doc_metadata)}")

        # Pre-validate PDF paths exist
        pdf_validation = {}
        for doc_id, doc_info in doc_metadata.items():
            filepath = doc_info.get("filepath", "")
            if filepath and filepath.lower().endswith(".pdf"):
                exists = Path(filepath).exists()
                pdf_validation[doc_id] = {"path": filepath, "exists": exists}
                if not exists:
                    logger.error(f"[EVIDENCE] PDF NOT FOUND: {doc_id} -> {filepath}")
                else:
                    logger.info(f"[EVIDENCE] PDF verified: {doc_id} -> {filepath}")

        if not pdf_validation:
            logger.error("[EVIDENCE] No PDF documents found in doc_metadata!")
            print(f"‚ö†Ô∏è  No PDF documents found for coordinate enrichment", flush=True)
        else:
            missing_pdfs = sum(1 for v in pdf_validation.values() if not v["exists"])
            if missing_pdfs > 0:
                logger.error(f"[EVIDENCE] {missing_pdfs}/{len(pdf_validation)} PDFs are missing!")
                print(f"‚ö†Ô∏è  {missing_pdfs} PDF(s) not found on disk", flush=True)

        # Enrich snippets with coordinates (mutates in-place)
        enriched_count = enrich_snippets_with_coordinates(
            snippets=all_snippets,
            doc_metadata=doc_metadata,
            min_similarity=0.8,
        )

        # Log enrichment results
        if enriched_count == 0 and len(all_snippets) > 0:
            logger.error(f"[EVIDENCE] ENRICHMENT FAILED: 0/{len(all_snippets)} snippets enriched!")
            print(f"‚ö†Ô∏è  Coordinate enrichment returned 0 results - check logs for details", flush=True)
        else:
            success_rate = (enriched_count / len(all_snippets) * 100) if all_snippets else 0
            logger.info(f"[EVIDENCE] Enrichment complete: {enriched_count}/{len(all_snippets)} ({success_rate:.1f}%)")
            print(f"‚úÖ Enriched {enriched_count}/{len(all_snippets)} snippets with coordinates ({success_rate:.0f}%)", flush=True)

    except ImportError as e:
        logger.error(f"[EVIDENCE] Import error during coordinate enrichment: {e}", exc_info=True)
        print(f"‚ö†Ô∏è  Coordinate enrichment unavailable: missing dependency", flush=True)
    except Exception as e:
        logger.error(f"[EVIDENCE] Coordinate enrichment failed: {e}", exc_info=True)
        print(f"‚ö†Ô∏è  Coordinate enrichment failed: {e}", flush=True)

    # ========================================================================
    # Phase 4: Calculate Statistics & Save
    # ========================================================================

    covered = sum(1 for e in all_evidence if e.status == "covered")
    partial = sum(1 for e in all_evidence if e.status == "partial")
    missing = sum(1 for e in all_evidence if e.status == "missing")

    overall_coverage = (covered + (partial * 0.5)) / len(all_evidence) if all_evidence else 0.0

    print(f"\n‚úÖ Evidence extraction complete:", flush=True)
    print(f"   ‚Ä¢ Covered: {covered} ({covered/len(requirements)*100:.0f}%)", flush=True)
    print(f"   ‚Ä¢ Partial: {partial} ({partial/len(requirements)*100:.0f}%)", flush=True)
    print(f"   ‚Ä¢ Missing: {missing} ({missing/len(requirements)*100:.0f}%)", flush=True)

    result = EvidenceExtractionResult(
        session_id=session_id,
        requirements_total=len(requirements),
        requirements_covered=covered,
        requirements_partial=partial,
        requirements_missing=missing,
        requirements_flagged=0,
        overall_coverage=overall_coverage,
        evidence=[e for e in all_evidence]
    )

    # Save to state
    state_manager.write_json("evidence.json", result.model_dump())

    # Update session workflow progress
    session_data = state_manager.read_json("session.json")
    session_data["workflow_progress"]["evidence_extraction"] = "completed"
    session_data["statistics"]["requirements_covered"] = covered
    session_data["statistics"]["overall_coverage"] = overall_coverage
    state_manager.write_json("session.json", session_data)

    return result.model_dump()
